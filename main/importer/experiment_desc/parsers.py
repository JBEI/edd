# coding: utf-8
from __future__ import unicode_literals

import json
import logging
import re
from collections import Sequence

from builtins import str
from django.conf import settings
from openpyxl.utils.cell import get_column_letter
from six import string_types

from .constants import (
    DUPLICATE_ASSAY_METADATA,
    INVALID_CELL_TYPE,
    INVALID_REPLICATE_COUNT,
    MISSING_REQUIRED_LINE_NAME,
    PARSE_ERROR,
    PART_NUMBER_PATTERN_UNMATCHED_WARNING,
    ROWS_MISSING_REPLICATE_COUNT,
    INVALID_COLUMN_HEADER,
    UNMATCHED_ASSAY_COL_HEADERS_KEY, MULTIPLE_WORKSHEETS_FOUND,
    UNSUPPORTED_LINE_METADATA, EMPTY_WORKBOOK, ZERO_REPLICATES,
    INCORRECT_TIME_FORMAT, UNPARSEABLE_COMBINATORIAL_VALUE,
    INTERNAL_EDD_ERROR_TITLE, BAD_FILE_CATEGORY,
    PART_NUM_PATTERN_TITLE, IGNORED_INPUT_CATEGORY,
    INVALID_FILE_VALUE_CATEGORY, BAD_GENERIC_INPUT_CATEGORY)
from .utilities import AutomatedNamingStrategy, CombinatorialDescriptionInput, NamingStrategy


logger = logging.getLogger(__name__)

TYPICAL_ICE_PART_NUMBER_PATTERN = settings.TYPICAL_ICE_PART_NUMBER_PATTERN

###################################################################################################
# Column header patterns for the experiment description file.
# These match values generated by the ICE bulk export and also consumed by EDD's bulk line creation
# script, so don't change them arbitrarily!
LINE_NAME_COL_LABEL = 'Line\s+Name'
LINE_DESCRIPTION_COL_REGEX = 'Line\s+Description'
STRAIN_IDS_COL_LABEL = 'Part\s+ID'
REPLICATE_COUNT_COL_REGEX = 'Replicate\s+Count'

_LINE_NAME_COL_PATTERN = re.compile(r'\s*%s\s*' % LINE_NAME_COL_LABEL, re.IGNORECASE)
_LINE_DESCRIPTION_COL_PATTERN = re.compile(r'\s*%s\s*' % LINE_DESCRIPTION_COL_REGEX, re.IGNORECASE)
_STRAIN_IDS_COL_PATTERN = re.compile(r'\s*%s\s*' % STRAIN_IDS_COL_LABEL, re.IGNORECASE)
_REPLICATE_COUNT_COL_PATTERN = re.compile(r'\s*%s\s*' % REPLICATE_COUNT_COL_REGEX)
###################################################################################################

_STRAIN_GROUPS_REGEX = r'\((:?\s*\d+\s*;?\s*)+\)'
_STRAIN_GROUPS_PATTERN = re.compile(_STRAIN_GROUPS_REGEX)

_TIME_VALUE_REGEX = r'^\s*(\d+(?:\.\d+)?)\s*h\s*$'
_TIME_VALUE_PATTERN = re.compile(_TIME_VALUE_REGEX, re.IGNORECASE)

# tests whether the input string ends with 's' or '(s)'
_PLURALIZED_REGEX = r'^%s(?:S|\(S\))$'


class _AssayMetadataValueParser(object):
    def parse(self, raw_value_str):
        """
        Parses the raw string input for a single assay metadata value
        :return the parsed value to store, or None if no value should be stored
        :raise ValueError if the value couldn't be parsed
        """
        raise ValueError("Choose one of the specific sub-classes to parse value")


class _RawStringValueParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        stripped = raw_value_str.strip()
        if not stripped:
            return None
        return stripped


class _DecimalTimeParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        match = _TIME_VALUE_PATTERN.match(raw_value_str)
        if match:
            str_value = match.group(1)
            number_value = float(str_value)  # raises ValueError as in the spec
            stripped = str(str_value.strip()).replace(',', '').replace('-', '').replace('+', '')
            sep_index = stripped.find('.')  # TODO: i18n

            fractional_digits = 0
            if sep_index >= 0:
                fractional_digits = (len(stripped) - sep_index) - 1  # TODO: commas!
            return number_value, fractional_digits
        raise ValueError(
            'Value "%s" did not match the expected time pattern (e.g. "4.0h")' % raw_value_str
        )

# stateless value parsing strategies for metadata input (time is treated specially by the file
# format)
RAW_STRING_PARSER = _RawStringValueParser()
TIME_PARSER = _DecimalTimeParser()


class ColumnLayout:
    """
    Stores column layout read from the header row of an experiment description file. Since these
    files are designed to be user edited, parsing should be as tolerant as possible.
    """

    def __init__(self, importer):
        self.line_name_col = None
        self.line_description_col = None
        self.line_control_col = None
        self.replicate_count_col = None
        self.strain_ids_col = None
        self.col_index_to_line_meta_pk = {}
        self.col_index_to_assay_data = {}  # maps col index -> (Protocol, MetadataType)
        self.combinatorial_col_indices = []  # indices of all metadata columns for combinatorial
        self.unique_assay_protocols = {}
        self.importer = importer

    def register_protocol(self, protocol):
        self.unique_assay_protocols[protocol.pk] = True

    def has_assay_metadata(self, upper_protocol_name, metadata_pk):
        """
        Tests whether any columns have been detected so far that store a specific (Protocol,
        MetadataType) pair.

        :param upper_protocol_name:
        :param metadata_pk:
        :return:
        """
        items = self.col_index_to_assay_data.iteritems()
        for col_index, (existing_protocol, existing_assay_meta_type) in items:
            if ((upper_protocol_name == existing_protocol.name.upper()) and
                    (metadata_pk == existing_assay_meta_type.pk)):
                return True
        return False

    def register_assay_meta_column(self, col_index, upper_protocol_name, protocol, assay_meta_type,
                                   is_combinatorial):

        # test for duplicate use of the same (Protocol, MetadataType) combination.
        # if we see it, log an error -- no clear/automated way for us to resolve which column
        #  has the correct values!
        if self.has_assay_metadata(upper_protocol_name, assay_meta_type.pk):
            self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_ASSAY_METADATA,
                                    assay_meta_type.pk)

        self.register_protocol(protocol)

        self.col_index_to_assay_data[col_index] = (protocol, assay_meta_type)
        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)

        logger.debug(
            'Column %(col)d matches protocol "%(protocol)s", assay metadata type "%(meta_type)s"' %
            {
                'col': col_index + 1,
                'protocol': protocol.name,
                'meta_type': assay_meta_type.type_name
            }
        )

    def get_assay_metadata_type(self, col_index):
        value = self.col_index_to_assay_data.get(col_index, None)
        if not value:
            return None

        return value[1]

    def get_line_metadata_type(self, col_index):
        return self.col_index_to_line_meta_pk.get(col_index, None)

    @property
    def unique_protocols(self):
        return self.unique_assay_protocols.keys()

    def set_line_metadata_type(self, col_index, line_metadata_type, is_combinatorial=False):
        logger.debug(
            'Column %d matches line metadata type %s' %
            (col_index+1, line_metadata_type.type_name)
        )

        self.col_index_to_line_meta_pk[col_index] = line_metadata_type.pk

        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)


class _ExperimentDescNamingStrategy(NamingStrategy):
    """
    A simple line/assay naming strategy assumed in the experiment description file use case,
    where line names are user-specified and assay names are created automatically by appending
    the time to the line name. Note that this allows for duplicate assay names within different
    protocols, which should be clear in EDD's UI from protocol filtering and unit markings in the
    visualizations.
    """

    def __init__(self, assay_time_metadata_type_pk):
        super(_ExperimentDescNamingStrategy, self).__init__()
        self.base_line_name = None
        self.assay_time_metadata_type_pk = assay_time_metadata_type_pk

    def get_line_name(self, strains, line_metadata, replicate_num, line_metadata_types,
                      combinatorial_metadata_types, is_control):

        # if making lines combinatorially based on line metadata, insert the combinatorial values
        # into the line name so that line names will be unique
        combinatorial_suffix = '-'.join([
            line_metadata[meta_pk].replace(' ', '_')
            for meta_pk in combinatorial_metadata_types.iterkeys()
        ])
        combinatorial_suffix = '-%s' % combinatorial_suffix if combinatorial_suffix else ''

        # if creating more than one replicate, build a suffix to show replicate number so that
        # line names are unique
        replicate_suffix = ''
        if self.combinatorial_input.replicate_count > 1:
            replicate_suffix = '-R%d' % replicate_num

        return '%(base_line_name)s%(combinatorial_suffix)s%(replicate_suffix)s' % {
            'base_line_name': self.base_line_name,
            'combinatorial_suffix': combinatorial_suffix,
            'replicate_suffix': replicate_suffix,
        }

    def _get_time_format_string(self):
        if self.fractional_time_digits:
            return '%0.' + ('%d' % self.fractional_time_digits + 'f')
        return '%d'

    def get_assay_name(self, line, protocol, assay_metadata, assay_metadata_types):
        try:
            time_hours = assay_metadata.get(self.assay_time_metadata_type_pk)
            time_str = self._get_time_format_string() % time_hours
            return '%(line_name)s-%(hours)sh' % {
                'line_name': line.name, 'hours': time_str,
            }
        except KeyError:
            raise ValueError(KeyError)  # raise more generic Exception published in the docstring


class _InputFileRow(CombinatorialDescriptionInput):
    """
    A special case of combinatorial line/assay creations in support of experiment description file
    upload. Each line of the file is itself a combinatorial line/assay creation, at
    least if protocols/times are included, or the more advanced combinatorial features used.
    One-line-per-row creation, which is what users will likely use templates for most
    often, is just a degenerate case of combinatorial creation.
    """

    def __init__(self, assay_time_meta_pk, row_number):
        super(_InputFileRow, self).__init__(_ExperimentDescNamingStrategy(assay_time_meta_pk))
        self.row_number = row_number

    @property
    def base_line_name(self):
        return self.naming_strategy.base_line_name

    @base_line_name.setter
    def base_line_name(self, name):
        self.naming_strategy.base_line_name = name




class CombinatorialInputParser(object):
    def __init__(self, protocols, line_metadata_types_by_pk, assay_metadata_types_by_pk):

        # if the metadata type is present in the database, construct a parser for assay time
        # (we need a pk to store it, and the parser
        assay_time_type = None
        for pk, metadata_type in assay_metadata_types_by_pk.items():
            if metadata_type.type_name.upper() == 'TIME':
                assay_time_type = metadata_type
                break

        self.assay_time_metadata_type_pk = assay_time_type.pk

    def parse(self, input_source, importer):
        raise NotImplementedError()  # require subclasses to implement


class ExperimentDescFileParser(CombinatorialInputParser):
    """
    File parser that takes a study "template file" as input and reads the contents into a list of
    CombinatorialCreationInput objects.
    """

    def __init__(self, protocols_by_pk, line_metadata_types_by_pk, assay_metadata_types_by_pk):
        super(ExperimentDescFileParser, self).__init__(protocols_by_pk, line_metadata_types_by_pk,
                                                       assay_metadata_types_by_pk)

        self.line_metadata_types_by_pk = line_metadata_types_by_pk

        # build a dict of Protocol name -> Protocol to simplify parsing
        self.protocols_by_name = {
            protocol.name.upper(): protocol
            for protocol_pk, protocol in protocols_by_pk.iteritems()
        }

        # build dicts that map each metadata type name -> MetaDataType to simplify parsing
        self.line_metadata_types_by_name = {
            meta.type_name.upper(): meta
            for pk, meta in line_metadata_types_by_pk.iteritems()
        }

        self.assay_metadata_types_by_name = {
            meta.type_name.upper(): meta
            for pk, meta in assay_metadata_types_by_pk.iteritems()
        }

        # build a list of line metadata types whose parsing isn't supported pending resolution of
        # EDD-438. Note that these *are* supported via JSON pk input, we just aren't supporting
        # lookup for now since it may be done for us, or will at least be impacted by EDD-438
        self.unsupported_line_meta_types_by_pk = {
            pk: meta for pk, meta in line_metadata_types_by_pk.iteritems() if meta.type_name in
            ['Control', 'Carbon Source(s)', 'Line Contact', 'Line Experimenter', ]
        }

        # print a warning for unlikely case-sensitivity-only metadata naming differences that
        # clash with tolerant case-insensitive matching of user input in the file (which is a lot
        # more likely to be inconsistent)
        if len(self.line_metadata_types_by_name) != len(line_metadata_types_by_pk):
            logger.warning(
                'Found some line metadata types that differ only by case. Case-insensitive '
                'matching in parsing code will arbitrarily choose one'
            )

        if len(self.assay_metadata_types_by_name) != len(assay_metadata_types_by_pk):
            logger.warning(
                'Found some assay metadata types that differ only by case. Case-insensitive '
                'matching in this function will arbitrarily choose one'
            )

        # Note: uniqueness of protocol names is enforced by Protocol.save()... we'll trust that
        # and not print a warning here.

        self.column_layout = None

        # true to treat all unmatched column headers as an
        # error. False to ignore unmatch line headers, but to treat those that start with a
        # protocol name and don't match an assay MetaDataType as an error
        self.REQUIRE_COL_HEADER_MATCH = True

        # if the metadata type is present in the database, construct a parser for assay time
        # (we need a pk to store it, and the parser
        assay_time_type = self.assay_metadata_types_by_name.get('TIME', None)
        self.assay_time_metadata_type_pk = assay_time_type.pk if assay_time_type else None

        self.importer = None

        self.max_fractional_time_digits = 0

    def parse(self, wb, importer):
        logger.warning('In parse(). workbook has %d sheets' % len(wb.worksheets))

        if len(wb.worksheets) == 0:
            importer.add_error(BAD_FILE_CATEGORY, subtitle=EMPTY_WORKBOOK)
            return

        # Clear out state from any previous use of this parser instance
        self.column_layout = None
        self.importer = importer

        # loop over rows
        parsed_row_inputs = []

        if len(wb.worksheets) > 1:
            sheet_name = wb.get_sheet_names[0]
            importer.add_warning(IGNORED_INPUT_CATEGORY, MULTIPLE_WORKSHEETS_FOUND,
                                 'All but the first sheet, "%(sheet_name)s", were ignored' % {
                                     'sheet_name': sheet_name,
                                 })
        worksheet = wb.worksheets[0]

        # loop over columns
        row_index = 0
        for cols_list in worksheet.iter_rows():

            logger.debug('Examining row %d' % (row_index+1))

            # identify columns of interest first by looking for required labels
            if not self.column_layout:
                self.column_layout = self.read_column_layout(cols_list)

            # if column labels have been identified, look for line creation input data
            else:
                #  remove
                row_num = row_index + 1
                row_inputs = self.read_row(cols_list, row_num)
                if row_inputs:
                    parsed_row_inputs.append(row_inputs)

            row_index += 1

        if not self.column_layout:
            importer.add_error(BAD_FILE_CATEGORY,
                               subtitle='No column header was found matching the single required '
                                        'value "Line Name"')
            return

        column_layout = self.column_layout

        # provide a good user-facing warning message as a reminder of line metadata types that
        # aren't supported, but were found in the input file
        unsupported_value_columns = [col_index for col_index, meta_pk in
                                     column_layout.col_index_to_line_meta_pk.iteritems()
                                     if meta_pk in self.unsupported_line_meta_types_by_pk]
        if unsupported_value_columns:
            unsupported_values = []
            for col_index in unsupported_value_columns:
                meta_pk = column_layout.get_line_metadata_type(col_index)
                meta_type = self.line_metadata_types_by_pk[meta_pk]
                value = '"%(name)s" (col "%(col)s")' % {
                            'name': meta_type.type_name,
                            'col': get_column_letter(col_index+1)}
                unsupported_values.append(value)

            importer.add_warning(IGNORED_INPUT_CATEGORY, UNSUPPORTED_LINE_METADATA,
                                 ', '.join(unsupported_values))

        for combinatorial_input in parsed_row_inputs:
            combinatorial_input.fractional_time_digits = self.max_fractional_time_digits

        return parsed_row_inputs

    def read_column_layout(self, row):
        """
        Detects the layout of a template file by matching cell contents of a row containing the
        minimal required column headers, then comparing additional headers in that row against line
        and assay metadata names in EDD's database. If required column headers aren't found in this
        row, then it is ignored. Columns can be provided in any order.
        :param row: the row to inspect for column headers
        :return: the column layout if required columns were found, or None otherwise
        """
        logger.debug('in read_column_layout()')  # TODO: remove
        layout = ColumnLayout(self)
        # TODO: add support for control column

        ###########################################################################################
        # loop over columns in the current row, looking for labels that identify at least the
        # minimum set of required columns
        ###########################################################################################
        found_col_labels = False
        for col_index in range(len(row)):
            cell_content = row[col_index].value

            # ignore non-string cells since they can't be the column headers we're looking for
            if not isinstance(cell_content, string_types):
                continue

            cell_content = cell_content.strip()

            # skip this cell if it has no non-whitespace content
            if not cell_content:
                continue

            #######################################################################################
            # check whether column label matches one of the fixed labels specified by the file
            # format
            #######################################################################################
            if _LINE_NAME_COL_PATTERN.match(cell_content):
                layout.line_name_col = col_index
            elif _LINE_DESCRIPTION_COL_PATTERN.match(cell_content):
                layout.line_description_col = col_index
            elif _STRAIN_IDS_COL_PATTERN.match(cell_content):
                layout.strain_ids_col = col_index
            elif _REPLICATE_COUNT_COL_PATTERN.match(cell_content):
                layout.replicate_count_col = col_index

            # check whether the column label matches custom data defined in the database
            else:
                upper_content = cell_content.upper()

                # test whether this column is protocol-prefixed assay metadata
                assay_meta_type = self._parse_assay_metadata_header(
                    layout,
                    upper_content,
                    col_index
                )
                # if we found the type of this column, proceed to the next
                if assay_meta_type:
                    continue
                # if this column isn't protocol-prefixed, test whether it's for line metadata
                line_metadata_type = self._parse_line_metadata_header(
                    layout,
                    upper_content,
                    col_index
                )

                # if we couldn't process this column, track a warning that describes
                # dropped columns (can be displayed later in the UI)
                if line_metadata_type is None:
                    skipped = '%(title)s (%(col)s)' % {
                        'title': cell_content,
                        'col': get_column_letter(col_index+1),
                    }
                    is_error = self.REQUIRE_COL_HEADER_MATCH
                    self.importer.add_issue(is_error, BAD_FILE_CATEGORY, INVALID_COLUMN_HEADER, skipped)

        # test whether we've located all the required columns
        found_col_labels = layout.line_name_col is not None

        # return the columns found in this row if at least the
        # minimum required columns were found
        if found_col_labels:
            return layout

        return None

    def _parse_assay_metadata_header(self, layout, upper_content, col_index):
        """
        :return: a truthy value if the content should be treated as assay metadata (the
            MetadataType if one was found, or True if it was clearly intended to be one, but was
            logged as an error).
        """

        ########################################################################################
        # loop over protocol names, testing for a protocol prefix in the column header
        ########################################################################################
        for upper_protocol_name, protocol in self.protocols_by_name.items():
            if upper_content.startswith(upper_protocol_name):

                # pull out the column header suffix following the protocol.
                # it should match the name of an assay metadata type
                assay_meta_suffix = upper_content[len(upper_protocol_name):].strip()

                suffix_meta_type = None
                is_combinatorial = False

                ################################################################################
                # loop over assay metadata types, testing for an assay metadata suffix in the
                # column header
                ################################################################################
                for assay_metadata_type in self.assay_metadata_types_by_name.itervalues():

                    # look for an exact match
                    suffix_meta_type = self.assay_metadata_types_by_name.get(assay_meta_suffix,
                                                                             None)

                    if suffix_meta_type is not None:
                        layout = self.column_layout
                        layout.register_assay_meta_column(
                            col_index,
                            upper_protocol_name,
                            protocol,
                            suffix_meta_type,
                            is_combinatorial
                        )
                        break

                    # if no exact match is found look for a pluralized version of the metadata
                    # type name. Pluralization indicates the contents should be treated as a
                    # comma-delimited list of combinatorial metadata values
                    meta_regex = _PLURALIZED_REGEX % re.escape(assay_metadata_type.type_name)
                    pluralized_match = re.match(meta_regex, assay_meta_suffix, re.IGNORECASE)

                    if pluralized_match:
                        is_combinatorial = True
                        logger.debug(
                            'column header suffix %s matched pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )
                        suffix_meta_type = assay_metadata_type
                        break
                    else:
                        logger.debug(
                            'column header suffix %s did not match pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )

                # if the column started with the name of a protocol and ended with an
                # assay metadata type name, store the association of this column with the
                # (Protocol, MetadataType) combination
                if suffix_meta_type:
                    layout.register_assay_meta_column(col_index, upper_protocol_name,
                                                      protocol, suffix_meta_type,
                                                      is_combinatorial)
                    return suffix_meta_type

                # otherwise, since the column header was prefixed with a valid
                # protocol name, assume there was a data entry error or missing metadata type
                # in the database. This check is especially important for the Time metadata
                # assumed by the file format.
                else:
                    col_letter = get_column_letter(col_index+1)
                    logger.debug("""Column header suffix "%s" didn't match known metadata types""")
                    self.importer.add_error(BAD_FILE_CATEGORY,
                                            UNMATCHED_ASSAY_COL_HEADERS_KEY, col_letter)
                    return True

    def _parse_line_metadata_header(self, column_layout, upper_content, col_index):
        """
        :return: the line MetadataType if one was found or None otherwise
        """

        line_metadata_types = self.line_metadata_types_by_name

        # test whether the cell content matches the name of a line metadata type
        line_metadata_type = line_metadata_types.get(upper_content, None)
        if line_metadata_type is not None:
            column_layout.set_line_metadata_type(col_index, line_metadata_type)
            return line_metadata_type

        # if we didn't find the singular form of the column header as line metadata, look
        # for a pluralized version that we'll treat as combinatorial line creation input
        for upper_metadata_type_name, meta_type in line_metadata_types.items():

            meta_regex = _PLURALIZED_REGEX % upper_metadata_type_name
            pluralized_match = re.match(meta_regex, upper_content)

            if pluralized_match:
                line_metadata_type = meta_type
                column_layout.set_line_metadata_type(col_index, line_metadata_type,
                                                     is_combinatorial=True)
                return line_metadata_type

        return None

    def read_row(self, cols_list, row_num):
        """
        Reads a single spreadsheet row to find line creation inputs. The row is read even if errors
        occur, logging errors in the 'errors' parameter so that multiple user input errors can be
        detected and communicated during a single pass of editing the file.
        :param layout: the column header layout read from the beginning of the file. Informs this
            method which optional columns have been defined, as well as what order the columns are
            in (arbitrary column order is supported).
        """
        row_inputs = _InputFileRow(self.assay_time_metadata_type_pk, row_num)
        layout = self.column_layout

        ###################################################
        # Line name
        ###################################################
        cell_content = self._get_string_cell_content(
            cols_list,
            row_num,
            layout.line_name_col,
            convert_to_string=True
        )

        if cell_content:
            row_inputs.base_line_name = cell_content

        # otherwise, track error state, but keep going so we can try to detect all the
        # errors with a single pass
        else:
            logger.debug(
                'Parse error: Cell %(row_num)d%(col)s was empty, but was expected '
                'to contain a name for the EDD line.' % {
                    'row_num': row_num,
                    'col': get_column_letter(layout.line_name_col + 1),
                }
            )
            self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, MISSING_REQUIRED_LINE_NAME,
                                    occurrence_detail=row_num)

        ###################################################
        # Line description
        ###################################################
        if layout.line_description_col is not None:
            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_description_col
            )
            if cell_content:
                row_inputs.description = cell_content

        ###################################################
        # Control
        ###################################################
        if layout.line_control_col is not None:

            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_control_col
            )

            if cell_content:
                tokens = cell_content.split(',')
                if len(tokens) == 1:
                    tokens = cell_content

                values = []
                for token in tokens:
                    is_control = "TRUE" == token or "YES" == cell_content
                    values.append(is_control)
                row_inputs.is_control = values

        ###################################################
        # Replicate count
        ###################################################
        if layout.replicate_count_col is not None:
            cell_content = cols_list[layout.replicate_count_col].value

            if cell_content is not None:
                try:
                    row_inputs.replicate_count = int(cell_content)
                    if row_inputs.replicate_count == 0:
                        cell = '%(row)d%(col)s' % {
                            'value': str(cell_content), 'row': row_num,
                            'col': get_column_letter(layout.replicate_count_col + 1),
                        }
                        self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, ZERO_REPLICATES, cell)
                except ValueError:
                    message = '%(value)s (%(row)d%(col)s)' % {
                        'value': str(cell_content),
                        'row': row_num,
                        'col': get_column_letter(layout.replicate_count_col+1),
                    }
                    self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, INVALID_REPLICATE_COUNT,
                                            occurrence_detail=message)
            else:
                row_inputs.replicate_count = 1
                self.importer.add_warning(IGNORED_INPUT_CATEGORY, ROWS_MISSING_REPLICATE_COUNT,
                                          row_num)

        ###################################################
        # Strain part id(s)
        ###################################################
        # TODO: after some initial testing, consider adding custom group support for Carbon Source
        # similar to that for strains. Since some changes to Carbon Source / Media tracking are
        # needed, we will probably want to defer support for Carbon Sources for now.
        # TODO: after using the combinatorial strain creation code here for some testing of other
        # parts of the back end, consider removing it since it will result in creation of lines
        # with duplicate names
        if layout.strain_ids_col is not None:

            cell_content = cols_list[layout.strain_ids_col].value

            # build a list of strain ids for this input
            individual_strain_ids = []

            if cell_content:
                tokens = cell_content.split(',')
                if tokens:

                    # loop over comma-delimited tokens included in the cell
                    for token in tokens:
                        token = token.strip()

                        # if this token is a paren-enclosed list of part numbers, it's a
                        # combinatorial strain creation group rather than single strain to be
                        # included in the list. That means that each top-level comma-delimited
                        # entry in the list will result in creation of at least one line
                        strain_group_match = _STRAIN_GROUPS_PATTERN.match(token)

                        if strain_group_match:
                            strain_group = (strain_id.strip() for strain_id in strain_group_match
                                            .group(1).split(';'))
                            row_inputs.combinatorial_strain_id_groups.append(strain_group)
                        else:
                            individual_strain_ids.append(token)

                        # test whether the strain's part number matched the expected pattern.
                        # we'll allow all input through to the ICE query later in case our pattern
                        # is dated, but this way we can provide a more helpful prompt for bad
                        # user input
                        part_number_match = TYPICAL_ICE_PART_NUMBER_PATTERN.match(token)

                        if not part_number_match:
                            self.importer.add_warning(PART_NUM_PATTERN_TITLE,
                                                      PART_NUMBER_PATTERN_UNMATCHED_WARNING, token)
                            logger.warning(
                                'Expected ICE part number(s) in template file row %(row_num)d, '
                                'but "%(token)s" did not match the expected pattern. This is '
                                'either bad user input, or indicates that the pattern needs '
                                'updating.' % {
                                    'row_num': row_num,
                                    'token': token
                                }
                            )

                    # resolve inconsistent user entries, if present. assumption is that if any
                    # strain groups were provided, even individual strains listed separately (
                    # i.e. with no enclosing parens) should be treated as 1-element combinatorial
                    #  strain groups TODO: need to resolve this at DB interaction time, since row
                    #  order can dictate results here
                    if row_inputs.combinatorial_strain_id_groups and individual_strain_ids:
                        for strain_id in individual_strain_ids:
                            row_inputs.combinatorial_strain_id_groups.append((strain_id,))
                        individual_strain_ids = []
                    elif individual_strain_ids:
                        row_inputs.combinatorial_strain_id_groups.append(individual_strain_ids)

        ###################################################
        # line metadata
        ###################################################
        if layout.col_index_to_line_meta_pk:
            for col_index, line_metadata_pk in layout.col_index_to_line_meta_pk.items():
                # skip values for metadata types we've specifically disabled (with a warning)
                # earlier in the parsing process
                if line_metadata_pk in self.unsupported_line_meta_types_by_pk:
                    continue

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index,
                    convert_to_string=True
                )

                if col_index in layout.combinatorial_col_indices:
                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        line_metadata_pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        UNPARSEABLE_COMBINATORIAL_VALUE,
                    )
                else:
                    row_inputs.add_common_line_metadata(line_metadata_pk, cell_content)

        ###################################################################################
        # loop over protocol-specific columns (most likely related assay measurement times)
        ###################################################################################
        if layout.col_index_to_assay_data:

            # loop over per-protocol assay metadata columns
            for col_index, (protocol, assay_metadata_type) in \
                    layout.col_index_to_assay_data.items():

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index, convert_to_string=True
                )

                # skip blank cells
                if not cell_content:
                    continue

                # if this cell is in a column of for combinatorial input, add it to that list
                if col_index in layout.combinatorial_col_indices:
                    is_time = assay_metadata_type.pk == self.assay_time_metadata_type_pk

                    error_key = (INCORRECT_TIME_FORMAT if is_time else
                                 UNPARSEABLE_COMBINATORIAL_VALUE)
                    parser = TIME_PARSER if is_time else RAW_STRING_PARSER

                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        assay_metadata_type.pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        error_key,
                        protocol,
                        parser
                    )

                else:
                    if row_inputs.has_assay_metadata_type(protocol.pk, assay_metadata_type.pk):
                        # TODO could improve this error content with a more complex data structure
                        self.importer.add_error(BAD_FILE_CATEGORY,
                                                DUPLICATE_ASSAY_METADATA,
                                                'col %(col_letter)s' % {
                                                    # 'protocol_id': protocol.pk,
                                                    # 'metadata_id': assay_metadata_type.pk,
                                                    'col_letter': get_column_letter(
                                                        col_index + 1),
                                                })
                    row_inputs.add_common_assay_metadata(
                        protocol.pk,
                        assay_metadata_type.pk,
                        cell_content
                    )

        return row_inputs

    def _get_string_cell_content(self, row, row_num, col_index, convert_to_string=False):
        cell_content = row[col_index].value

        if cell_content is None:
            return cell_content

        if isinstance(cell_content, string_types):
            return cell_content.strip()

        else:
            actual_type = type(cell_content).__name__
            if convert_to_string:
                logger.warning('Converted non-string data of type %s to string' % actual_type)
                return str(cell_content)
            else:
                msg = '%(row)d%(col)s (value: %(value)s, type: %(type)s)' % {
                    'row': row_num,
                    'col': get_column_letter(col_index+1),
                    'type': actual_type,
                    'value': cell_content,
                }
                self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, INVALID_CELL_TYPE, msg)
                return None

    def _parse_combinatorial_input(self, row_inputs, cell_content, row_num, col_index,
                                   metadata_type_pk, error_category, error_key, protocol=None,
                                   value_parser=RAW_STRING_PARSER):
        """
        Parses the value of a single cell that may / may not have comma-separated combinatorial
        content.
        :param row_inputs:
        :param cell_content:
        :param row_num:
        :param col_index:
        :param metadata_type_pk:
        :param error_key:
        :param protocol:
        :param value_parser:
        :return:
        """

        for token in cell_content.split(','):
            token = token.strip()
            try:

                if value_parser == TIME_PARSER:
                    (parsed_value, fractional_digit_count) = value_parser.parse(token)
                    self.max_fractional_time_digits = max(self.max_fractional_time_digits,
                                                          fractional_digit_count)
                else:
                    parsed_value = value_parser.parse(token)

                if protocol:
                    row_inputs.add_combinatorial_assay_metadata(protocol.pk, metadata_type_pk,
                                                                parsed_value)
                else:
                    row_inputs.add_combinatorial_line_metadata(metadata_type_pk,
                                                               parsed_value)
            except ValueError:
                cell_num = '%(row_num)d%(col_letter)s' % {
                    'row_num': row_num,
                    'col_letter': get_column_letter(col_index+1)}
                logger.warning(
                    'ValueError parsing token "%(token)s" from cell content "%(value)s" in '
                    '%(cell)s' % {
                        'token': token,
                        'value': cell_content,
                        'cell': cell_num,
                    }
                )
                bad_value = '%(token)s ((cell_num)s)' % {
                    'token': token,
                    'cell_num': cell_num,
                }
                self.importer.add_error(error_category, error_key, bad_value)
                break


class JsonInputParser(CombinatorialInputParser):
    """
    Parses / verifies JSON input for combinatorial line creation. Note that instances of this
    class maintain a cache of protocols and metadata types defined in the database, so the
    lifecycle of each instance should be short.
    """
    def __init__(self, protocols_by_pk, line_metadata_types_by_pk, assay_metadata_types_by_pk):
        super(JsonInputParser, self).__init__(protocols_by_pk, line_metadata_types_by_pk,
                                              assay_metadata_types_by_pk)
        self.protocols_by_pk = protocols_by_pk
        self.line_metadata_types_by_pk = line_metadata_types_by_pk
        self.assay_metadata_types_by_pk = assay_metadata_types_by_pk
        self.max_fractional_time_digits = 0
        self.parsed_json = None

    def parse(self, stream, importer):

        combinatorial_inputs = []
        self.importer = importer

        if importer.errors:
            return None

        ###########################################################################################
        # Once validated, parse the JSON string into a Python dict or list
        # if validation succeeded, extract the naming strategy from the JSON, then pass the rest
        # as parameters to CombinatorialDescriptionInput. Also cache for possible later use by
        # client code (e.g. in err emails)

        parsed_json = json.loads(stream)
        self.parsed_json = parsed_json

        if not parsed_json:
            return None

        max_decimal_digits = 0

        # tolerate either a sequence of CombinatorialDescriptionInput or a single one
        if not isinstance(parsed_json, Sequence):
            parsed_json = [parsed_json]

        for value in parsed_json:

            # work around reserved 'description' keyword in JSON schema
            description = value.pop('desc', None)

            # convert string-based keys required by JSON into their numeric equivalents
            # TODO: consider casting values too
            common_line_metadata = _copy_to_numeric_keys(value.pop('common_line_metadata', {}))
            combinatorial_line_metadata = _copy_to_numeric_keys(
                    value.pop('combinatorial_line_metadata', {}))
            protocol_to_assay_metadata = _copy_to_numeric_keys(
                    value.pop('protocol_to_assay_metadata', {}))
            protocol_to_combinatorial_metadata = _copy_to_numeric_keys(
                    value.pop('protocol_to_combinatorial_metadata', {}))

            naming_strategy = None
            naming_elements = value.pop('name_elements', None)
            if naming_elements:
                naming_strategy = AutomatedNamingStrategy(
                    self.line_metadata_types_by_pk,
                    self.assay_metadata_types_by_pk,
                    self.assay_time_metadata_type_pk
                )
                elements = _copy_to_numeric_elts(naming_elements['elements'])
                abbreviations = _copy_to_numeric_keys(naming_elements['abbreviations'])

                naming_strategy.elements = elements
                naming_strategy.abbreviations = abbreviations
                naming_strategy.verify_naming_elts(importer)
            else:
                base_name = value.pop('base_name')
                naming_strategy = _ExperimentDescNamingStrategy(self.assay_time_metadata_type_pk)
                naming_strategy.base_line_name = base_name

            try:
                # just pass the JSON as initializer arguments. Won't verify the internal
                # structure/expected data types, but for starters that's probably a safe bet
                combo_input = CombinatorialDescriptionInput(
                    naming_strategy, description=description,
                    common_line_metadata=common_line_metadata,
                    combinatorial_line_metadata=combinatorial_line_metadata,
                    protocol_to_assay_metadata=protocol_to_assay_metadata,
                    protocol_to_combinatorial_metadata=protocol_to_combinatorial_metadata,
                    **value
                )

                # inspect JSON input to find the maximum number of decimal digits in the user input
                if self.assay_time_metadata_type_pk:
                    for protocol, assay_metadata in protocol_to_assay_metadata.items():
                        time_values = assay_metadata.get(self.assay_time_metadata_type_pk, [])
                        for time_value in time_values:
                            str_value = str(time_value)
                            if str_value != str((int(float(time_value)))):
                                decimal_digits = len(str_value) - str_value.find('.') - 1
                                max_decimal_digits = max(max_decimal_digits, decimal_digits)

                naming_strategy.combinatorial_input = combo_input
                combinatorial_inputs.append(combo_input)
            except RuntimeError as rte:
                logger.exception('Unexpected parse error')
                self.importer.add_error(INTERNAL_EDD_ERROR_TITLE, PARSE_ERROR, str(rte))

        # verify primary key inputs from the JSON are for the expected MetaDataType context,
        # and that they exist, since there's no runtime checking for this at database item
        # creation time when they get (necessarily) shoved into the hstore field. Note that this is
        # an non-ideal comparison of database fields cached in memory, but should be an acceptable
        # risk of having a stale cache of these values, which should be recently gathered and very
        # unlikely to be stale.

        for combo_input in combinatorial_inputs:
            combo_input.verify_pks(
                self.line_metadata_types_by_pk,
                self.assay_metadata_types_by_pk,
                self.protocols_by_pk,
                self.importer,
            )

        # TODO: verify ICE strains are provided for every input if required
        # if self.require_strains:
        #     for combo_input in combinatorial_inputs:
        #         if not combo_input.combinatorial_strain_id_groups:
        #             add_parse_error(errors, 'strains required for all lines')
        #
        #         elif isinstance(combo_input.combinatorial_strain_id_groups, Sequence):
        #             for strain_id_group in combo

        # consistently use decimal or integer time in assay names based on whether any fractional
        # input was provided
        for combo_input in combinatorial_inputs:
            combo_input.fractional_time_digits = max_decimal_digits

        return combinatorial_inputs


def _copy_to_numeric_elts(input_list):
    converted_list = []
    for index, element in enumerate(input_list):
        try:
            int_value = int(element)
            converted_list.append(int_value)
        except ValueError:
            converted_list.append(element)

    return converted_list


def _copy_to_numeric_keys(input_dict):
    converted_dict = {}
    for key, value in input_dict.iteritems():

        # if value is a nested dict, do the same work on it
        if isinstance(value, dict):
            value = _copy_to_numeric_keys(value)
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict
