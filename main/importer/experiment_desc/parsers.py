# coding: utf-8

import csv
import json
import logging
import re

from collections import defaultdict, Sequence
from jsonschema import Draft4Validator
from openpyxl import load_workbook
from openpyxl.utils.cell import get_column_letter
from six import string_types

from .constants import (
    ABBREVIATIONS_SECTION,
    BAD_FILE_CATEGORY,
    BAD_GENERIC_INPUT_CATEGORY,
    BASE_NAME_ELT,
    COMBINATORIAL_LINE_METADATA_SECTION,
    COMMON_LINE_METADATA_SECTION,
    DELIMETER_NOT_ALLOWED_VALUE,
    DUPLICATE_ASSAY_METADATA,
    DUPLICATE_LINE_NAME_LITERAL,
    DUPLICATE_LINE_ATTR,
    DUPLICATE_LINE_METADATA,
    ELEMENTS_SECTION,
    EMPTY_WORKBOOK,
    ICE_FOLDERS_KEY,
    IGNORED_INPUT_CATEGORY,
    INCONSISTENT_FILTERS,
    INCONSISTENT_FOLDERS,
    INCONSISTENT_COMBINATORIAL_VALUE,
    INCORRECT_TIME_FORMAT,
    INTERNAL_EDD_ERROR_CATEGORY,
    INVALID_CELL_TYPE,
    INVALID_COLUMN_HEADER,
    INVALID_FILE_VALUE_CATEGORY,
    INVALID_JSON,
    INVALID_REPLICATE_COUNT,
    MISSING_REQUIRED_LINE_NAME,
    MULTIPLE_WORKSHEETS_FOUND,
    NAME_ELEMENTS_SECTION,
    PARSE_ERROR,
    PART_NUMBER_PATTERN_UNMATCHED_WARNING,
    POSSIBLE_USER_ERROR_CATEGORY,
    PROTOCOL_TO_ASSAY_METADATA_SECTION,
    PROTOCOL_TO_COMBINATORIAL_METADATA_SECTION,
    REPLICATE_COUNT_ELT,
    ROWS_MISSING_REPLICATE_COUNT,
    UNMATCHED_ASSAY_COL_HEADERS_KEY,
    UNPARSEABLE_COMBINATORIAL_VALUE,
    UNSUPPORTED_LINE_METADATA,
    ZERO_REPLICATES,
)
from .utilities import (
    ALLOWED_RELATED_OBJECT_FIELDS,
    AutomatedNamingStrategy,
    CombinatorialDescriptionInput,
    NamingStrategy
)
from jbei.utils import TYPICAL_JBEI_ICE_PART_NUMBER_REGEX
from main.importer.experiment_desc.validators import SCHEMA as JSON_SCHEMA


logger = logging.getLogger(__name__)

TYPICAL_ICE_PART_NUMBER_PATTERN = re.compile(TYPICAL_JBEI_ICE_PART_NUMBER_REGEX, re.IGNORECASE)

###################################################################################################
# Column header patterns for the experiment description file.
# These match values generated by the ICE bulk export and also consumed by EDD's bulk line creation
# script, so don't change them arbitrarily!
LINE_NAME_COL_LABEL = 'Line\s+Name'
LINE_DESCRIPTION_COL_REGEX = 'Line\s+Description'
STRAIN_IDS_COL_LABEL = 'Part\s+ID'
REPLICATE_COUNT_COL_REGEX = 'Replicate\s+Count'

_LINE_NAME_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_NAME_COL_LABEL, re.IGNORECASE)
_LINE_DESCRIPTION_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_DESCRIPTION_COL_REGEX,
                                           re.IGNORECASE)
_STRAIN_IDS_SINGULAR_COL_PATTERN = re.compile(r'^\s*%s\s*$' % STRAIN_IDS_COL_LABEL, re.IGNORECASE)
_STRAIN_IDS_PLURAL_COL_PATTERN = re.compile(r'^\s*%s\s*\(s\)$' % STRAIN_IDS_COL_LABEL,
                                            re.IGNORECASE)
_REPLICATE_COUNT_COL_PATTERN = re.compile(r'^\s*%s\s*$' % REPLICATE_COUNT_COL_REGEX, re.IGNORECASE)
###################################################################################################

_STRAIN_GROUP_MEMBER_DELIM = ';'
_STRAIN_GROUP_REGEX = r'^\s*\(((?:\s*[^' + _STRAIN_GROUP_MEMBER_DELIM + '\)\(]+\s*' + \
                      _STRAIN_GROUP_MEMBER_DELIM + '?\s*)+)\)\s*$'
_STRAIN_GROUP_PATTERN = re.compile(_STRAIN_GROUP_REGEX)

_TIME_VALUE_REGEX = r'^\s*(\d+(?:\.\d+)?)\s*h\s*$'
_TIME_VALUE_PATTERN = re.compile(_TIME_VALUE_REGEX, re.IGNORECASE)

# tests whether the input string ends with 's' or '(s)'
_OPT_UNIT_SUFFIX = r'(?:\s*(?:\(%(units)s\)|%(units)s))?'
_TYPE_NAME_REGEX = r'^%(type_name)s' + _OPT_UNIT_SUFFIX + '$'
_PLURALIZED_REGEX = r'^%(type_name)s(?:S|\(S\))' + _OPT_UNIT_SUFFIX + '$'

_WHITESPACE_PATTERN = re.compile(r'\s+')


class _AssayMetadataValueParser(object):
    def parse(self, raw_value_str):
        """
        Parses the raw string input for a single assay metadata value
        :return the parsed value to store, or None if no value should be stored
        :raise ValueError if the value couldn't be parsed
        """
        raise ValueError("Choose one of the specific sub-classes to parse value")


class _RawStringValueParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        stripped = raw_value_str.strip()
        if not stripped:
            return None
        return stripped


class _DecimalTimeParser(_AssayMetadataValueParser):
    def __init__(self, parser):
        self.parser = parser

    def parse(self, raw_value_str):
        match = _TIME_VALUE_PATTERN.match(raw_value_str)
        if match:
            str_value = match.group(1)
            number_value = float(str_value)  # raises ValueError as in the spec
            stripped = str(str_value.strip()).replace(',', '').replace('-', '').replace('+', '')
            sep_index = stripped.find('.')  # TODO: i18n

            # maintain the count of fractional time digits used in the input so we can propagate
            # the same precision to resulting line names
            fractional_digits = 0
            if sep_index >= 0:
                fractional_digits = (len(stripped) - sep_index) - 1  # TODO: commas!

            parser = self.parser
            parser.max_fractional_time_digits = max(parser.max_fractional_time_digits,
                                                    fractional_digits)
            return number_value
        raise ValueError(f'Value "{raw_value_str}" did not match the expected time pattern (e.g. '
                         f'"4.0h")')


# stateless value parsing strategy for metadata input
_RAW_STRING_PARSER = _RawStringValueParser()


def cell_coords(row_index, col_index):
    col_letter = get_column_letter(col_index + 1)
    return f'{col_letter}{row_index+1}'


def header_desc(cell_content, col_index):
    col_letter = get_column_letter(col_index + 1)
    return f'"{cell_content}" (col {col_letter})'


def cell_desc(cell_content, row_index, col_index):
    coords = cell_coords(row_index, col_index)
    return f'"{cell_content}" ({coords})'


class ColumnLayout:
    """
    Stores column layout read from the header row of an experiment description file. Since these
    files are designed to be user edited, parsing should be as tolerant as possible.
    """

    def __init__(self, importer):
        self.line_name_col = None
        self.line_description_col = None
        self.line_control_col = None
        self.replicate_count_col = None
        self.strain_ids_col = None
        self.col_index_to_line_meta_pk = {}
        self.col_index_to_assay_data = {}  # maps col index -> (Protocol, MetadataType)

        # indices of all *any* columns for combinatorial creation (both metadata AND strains!)
        self.combinatorial_col_indices = []
        self.unique_assay_protocols = {}
        self.importer = importer

        # primary keys of line metadata types whose cols have already been detected in the file
        self.obs_line_meta_pks = set()

    def register_protocol(self, protocol):
        self.unique_assay_protocols[protocol.pk] = True

    def has_assay_metadata(self, upper_protocol_name, metadata_pk):
        """
        Tests whether any columns have been detected so far that store a specific (Protocol,
        MetadataType) pair.

        :param upper_protocol_name:
        :param metadata_pk:
        :return:
        """
        items = self.col_index_to_assay_data.items()
        for col_index, (existing_protocol, existing_assay_meta_type) in items:
            if ((upper_protocol_name == existing_protocol.name.upper()) and
                    (metadata_pk == existing_assay_meta_type.pk)):
                return True
        return False

    def combinatorial_line_col_order(self):
        return filter(
            lambda x: x in self.col_index_to_line_meta_pk,
            self.combinatorial_col_indices
        )

    def combinatorial_assay_col_order(self):
        return filter(
            lambda x: x in self.col_index_to_assay_data,
            self.combinatorial_col_indices
        )

    def register_assay_meta_column(self, col_header, col_index, upper_protocol_name, protocol,
                                   assay_mtype, is_combinatorial):

        # test for duplicate use of the same (Protocol, MetadataType) combination.
        # if we see it, log an error -- no clear/automated way for us to resolve which column
        #  has the correct values!
        if self.has_assay_metadata(upper_protocol_name, assay_mtype.pk):
            self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_ASSAY_METADATA,
                                    header_desc(col_header, col_index))

        self.register_protocol(protocol)
        self.col_index_to_assay_data[col_index] = (protocol, assay_mtype)
        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)

        col_letter = get_column_letter(col_index+1)
        logger.debug(f'Column {col_letter} matches protocol "{protocol.name}", assay metadata type'
                     f' "{assay_mtype.type_name}"')

    def get_assay_metadata_type(self, col_index):
        value = self.col_index_to_assay_data.get(col_index, None)
        if not value:
            return None

        return value[1]

    def get_line_meta_pk(self, col_index):
        return self.col_index_to_line_meta_pk.get(col_index, None)

    def get_meta_pk(self, col_index):
        """
        Gets the integer primary key of the Line OR Assay MetadataType the specified ED file column
        :param col_index: the column index
        :return: the MetadataType primary key
        """
        pk = self.get_line_meta_pk(col_index)
        if pk:
            return pk
        assay_mtype = self.get_assay_metadata_type(col_index)
        if not assay_mtype:
            return None
        return assay_mtype.pk

    @property
    def unique_protocols(self):
        return set(self.unique_assay_protocols)

    def register_line_meta_col(self, col_index, line_mtype, is_combinatorial=False):
        col_letter = get_column_letter(col_index + 1)
        logger.debug(f'Column {col_letter} matches line metadata type {line_mtype.type_name}')

        # detect duplicate columns in the file & add helpful err messages
        if line_mtype.pk in self.obs_line_meta_pks:
            self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_LINE_METADATA,
                                    header_desc(line_mtype.type_name, col_index))

        self.col_index_to_line_meta_pk[col_index] = line_mtype.pk
        self.obs_line_meta_pks.add(line_mtype.pk)

        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)


class _ExperimentDescNamingStrategy(NamingStrategy):
    """
    A simple line/assay naming strategy assumed in the experiment description file use case,
    where line names/assay names are created automatically by from a combination of the base
    line name, plus metadata values for any combinatorially-defined columns that are needed to
    make resulting line/assay names unique. Combinatorial metadata values included in the names are
    added in the order that columns were specified in the file.
    Note that this allows for duplicate assay names within different
    protocols, which should be clear in EDD's UI from protocol filtering and unit markings in the
    visualizations.
    """

    def __init__(self, col_layout, cache, importer):
        super(_ExperimentDescNamingStrategy, self).__init__(cache, importer)
        self.col_layout = col_layout
        self.base_line_name = None

    def get_line_name(self, line_metadata, replicate_num):
        """
        Computes the line name, either by using the explicitly-provided name from the file, OR if
        there are combinatorially-defined columns (by appending an 's' or '(s)' to the column
        header), by iterating over combinatorial columns in the order defined by the file,
        then appending combinatorial metadata values to the line name.  Note that if used,
        replicate number is always at the end regardless of column order.
        """
        layout = self.col_layout
        line_metadata_types = self.cache.line_meta_types
        cache = self.cache

        name_elts = []
        included_base_name = False

        # build the name segment for strains needed to make this line name unique (if any)
        # iterate over combinatorial line metadata columns and construct line name in the same
        # order that name-relevant elements were listed in columns in the input file. We have to
        # include values for the combinatorial metadata so that line names will be unique
        for line_metadata_col in layout.combinatorial_line_col_order():
            line_meta_pk = self.col_layout.get_line_meta_pk(line_metadata_col)

            # if the base line name hasn't been included yet, and comes before this metadata
            # element, insert it
            if (not included_base_name) and (layout.line_name_col < line_metadata_col):
                included_base_name = True
                name_elts.append(self.base_line_name.replace(' ', self.space_replacement))

            # do special processing for Line related object fields needed as part of computing
            # the line name
            if line_meta_pk in cache.related_object_mtypes:
                # as a functional stopgap, just hard-code the single supported related field
                # value and assume that's what was provided...worst case we'll just expose
                # the wrong (allowed) value
                meta_type = cache.line_meta_types[line_meta_pk]
                related_obj_field = ALLOWED_RELATED_OBJECT_FIELDS[meta_type.type_field]
                col_letter = get_column_letter(line_metadata_col)
                segment = self.build_related_objects_name_segment(line_metadata,
                                                                  line_meta_pk,
                                                                  related_obj_field,
                                                                  col_letter)
                name_elts.append(segment)
                logger.debug(f'Built related object name segment "{meta_type}"')
                continue

            metadata_value = line_metadata.get(line_meta_pk, None)  # value is optional!

            if not metadata_value:
                # TODO: add a warning that line names won't be consistent
                continue

            line_meta_type = line_metadata_types[line_meta_pk]
            naming_elt = metadata_value
            if line_meta_type.postfix:
                naming_elt += line_meta_type.postfix
            name_elts.append(naming_elt.replace(' ', self.space_replacement))

        # if the base line name still isn't added, add it
        if not included_base_name:
            name_elts.append(self.base_line_name)
            included_base_name = True

        # if creating more than one replicate, build a suffix to show replicate number so that
        # line names are unique. Replicate number should always be last in the line name
        if self.combinatorial_input.replicate_count > 1:
            replicate_suffix = f'R{replicate_num}'
            name_elts.append(replicate_suffix)

        logger.debug(f'Building line name from elements: {name_elts}')

        # if making lines combinatorially based on line metadata, insert the combinatorial values
        # into the line name so that line names will be unique
        name = self.section_separator.join(name_elts)
        return name

    def get_required_naming_meta_pks(self):
        """
        Gets the Line MetadataTypes that represent categories of data required for computing
        line names during the automated line creation process.  E.g. any data specified
        combinatorially.
        :return: set of integer primary keys for Line MetadataTypes
        """
        col_layout = self.col_layout
        meta_pks = set()

        for col in col_layout.combinatorial_col_indices:
            line_meta_pk = col_layout.col_index_to_line_meta_pk.get(col)
            if line_meta_pk:
                meta_pks.add(line_meta_pk)
        return meta_pks

    def _get_time_format_string(self):
        if self.fractional_time_digits:
            return '%0.' + ('%d' % self.fractional_time_digits + 'f')
        return '%d'

    def get_assay_name(self, line, protocol_pk, assay_metadata):
        layout = self.col_layout
        name_elts = [line.name]
        assay_time_mtype_pk = self.cache.assay_time_mtype.pk
        col_order = ','.join([str(index) for index in layout.combinatorial_assay_col_order()])
        logger.debug(f'Combinatorial assay column order: {col_order}')

        try:

            # iterate over combinatorial assay metadata columns and construct assay name in the
            # same order that name-relevant elements were listed in columns in the input file.
            # We have to include values for the combinatorial metadata so that assay names will
            # be unique
            for assay_metadata_col in layout.combinatorial_assay_col_order():
                col_protocol, assay_meta_type = layout.col_index_to_assay_data[assay_metadata_col]

                # if this column contained metadata for a different protocol, skip it
                if col_protocol.pk != protocol_pk:
                    continue

                col = get_column_letter(assay_metadata_col + 1)
                logger.debug(f'Inspecting combinatorial assay metadata column {col} for protocol '
                             f'"{col_protocol.name}, meta "{assay_meta_type.type_name}"')

                metadata_value = assay_metadata.get(assay_meta_type.pk)

                # TODO: some code in this block is essentially a workaround for missing units
                # in EDD's metadata types.  Can remove this later if they're updated to use
                # consistent units following EDD-741.
                name_elt = None
                if assay_meta_type.pk == assay_time_mtype_pk:
                    custom_time_digits = self._get_time_format_string() % metadata_value
                    name_elt = f'{custom_time_digits}h'
                else:
                    name_elt = metadata_value.replace(' ', self.space_replacement)

                    # add in units if defined...otherwise, multiple numeric values are
                    # hard/impossible to distinguish from each other
                    if assay_meta_type.postfix:
                        name_elt += assay_meta_type.postfix
                name_elts.append(name_elt)

            elts = ', '.join(name_elts)
            logger.debug(f'Adding assay naming elements: {elts}')
            return self.section_separator.join(name_elts)

        except KeyError:
            raise ValueError(KeyError)  # raise more generic Exception published in the docstring


class _ExperimentDescriptionFileRow(CombinatorialDescriptionInput):
    """
    A special case of combinatorial line/assay creations in support of experiment description file
    upload. Each line of the file is itself a combinatorial line/assay creation, at
    least if protocols/times are included, or the more advanced combinatorial features used.
    One-line-per-row creation, which is what users will likely use templates for most
    often, is just a degenerate case of combinatorial creation.
    """

    def __init__(self, column_layout, cache, row_number, importer, ):
        super(_ExperimentDescriptionFileRow, self).__init__(
            _ExperimentDescNamingStrategy(column_layout, cache, importer), importer)
        self.row_number = row_number

    @property
    def base_line_name(self):
        return self.naming_strategy.base_line_name

    @base_line_name.setter
    def base_line_name(self, name):
        self.naming_strategy.base_line_name = name


class CombinatorialInputParser(object):
    def __init__(self, cache, aggregator=None):
        self.cache = cache
        self.aggregator = aggregator


def _standardize_label(label):
    """
    Standardizes labeling to enable fast / flexible matching of Experiment Description column
    headers against the Protocol and MetadataType names defined in EDD's database.

    :param label: the original label.
    :return: the input text, capitalized, trimmed, and with consecutive whitespace characters
        collapsed to a single space.
    """
    return _WHITESPACE_PATTERN.sub(' ', label).strip().upper()


class ExperimentDescFileParser(CombinatorialInputParser):
    """
    File parser that takes an Experiment Description file as input and reads the contents into a
    list of CombinatorialCreationInput objects.
    """

    def __init__(self, cache, aggregator=None):
        super(ExperimentDescFileParser, self).__init__(cache)

        self._time_parser = _DecimalTimeParser(self)

        # build a dict of Protocol name -> Protocol to simplify parsing
        self.protocols_by_name = {
            protocol.name.upper(): protocol
            for protocol_pk, protocol in cache.protocols.items()
        }

        # build dicts that map each metadata type name -> MetaDataType to simplify parsing.
        line_meta_types = cache.line_meta_types
        self.line_mtypes_by_name = {
            _standardize_label(meta.type_name): meta
            for pk, meta in line_meta_types.items()
        }

        self.assay_metadata_types_by_name = {
            _standardize_label(meta.type_name): meta
            for pk, meta in cache.assay_meta_types.items()
        }

        # build a list of line metadata types whose parsing isn't supported pending resolution of
        # EDD-438. Note that these *are* supported via JSON pk input, we just aren't supporting
        # lookup for now since it may be done for us, or will at least be impacted by EDD-438
        unsupported_names = [
            'Control',
            'Carbon Source(s)',
            'Line Contact',
            'Line Experimenter',
        ]
        self.unsupported_line_meta_types_by_pk = {
            pk: meta
            for pk, meta in line_meta_types.items()
            if meta.type_name in unsupported_names
        }

        # print a warning for unlikely case-sensitivity-only metadata naming differences that
        # clash with tolerant case-insensitive matching of user input in the file (which is a lot
        # more likely to be inconsistent)
        if len(self.line_mtypes_by_name) != len(line_meta_types):
            logger.warning(
                'Found some line metadata types that differ only by case. Case-insensitive '
                'matching in parsing code will arbitrarily choose one'
            )

        if len(self.assay_metadata_types_by_name) != len(cache.assay_meta_types):
            logger.warning(
                'Found some assay metadata types that differ only by case. Case-insensitive '
                'matching in this function will arbitrarily choose one'
            )

        # Note: uniqueness of protocol names is enforced by Protocol.save()... we'll trust that
        # and not print a warning here.

        # get specific metadata references needed by current parsing code
        self.ctrl_meta_type = next(
            (x for x in cache.line_meta_types.values() if x.type_name == 'Control'),
            None  # default value
        )
        self.name_meta_type = next(
            (x for x in cache.line_meta_types.values() if x.type_name == 'Line Name'),
            None  # default value
        )

        self.column_layout = None

        self.importer = aggregator

        self.max_fractional_time_digits = 0

    # TODO: revisit to unify IO use & exception API across excel / csv methods.  At this point at
    # least, we have both working, if not as abstract/consistent as possible
    def parse_excel(self, file):
        # TODO: verify or remove docstring referencing ParseError
        """
        :param file: a path-like object identifying the file location
        :param options:
        :return:
        :raise OSError if the file can't be opened or ParseError if the file format or content
        is malformed.
        """
        self._is_excel = True

        wb = load_workbook(file, read_only=True, data_only=True)

        if len(wb.worksheets) == 0:
            self.importer.add_error(BAD_FILE_CATEGORY, subtitle=EMPTY_WORKBOOK)
            return  # TODO: raise an Exception

        if len(wb.worksheets) > 1:
            sheet_name = wb.get_sheet_names()[0]
            self.importer.add_warning(
                IGNORED_INPUT_CATEGORY, MULTIPLE_WORKSHEETS_FOUND,
                f'All but the first sheet in your workbook, "{sheet_name}", were ignored'
            )
        worksheet = wb.worksheets[0]
        return self._parse(worksheet.iter_rows())

    def parse_csv(self, lines_iter):
        self._is_excel = False
        reader = csv.reader(lines_iter)
        return self._parse(reader)

    def _parse(self, rows_iter):
        # Clear out state from any previous use of this parser instance
        self.column_layout = None
        parsed_row_inputs = []
        importer = self.importer

        # loop over rows
        for row_index, cols_list in enumerate(rows_iter):
            logger.debug(f'Parsing row {row_index+1}')

            # identify columns of interest first by looking for required labels
            if not self.column_layout:
                self.column_layout = self._parse_column_layout(cols_list)

            # if column labels have been identified, look for line creation input data
            else:
                row_inputs = self._parse_row(cols_list, row_index)
                if row_inputs:  # skip empty rows
                    parsed_row_inputs.append(row_inputs)

        column_layout = self.column_layout
        if not column_layout:
            importer.add_error(BAD_FILE_CATEGORY,
                               subtitle='No column header was found matching the single required '
                                        'value "Line Name"')
            return

        # provide a good user-facing warning message as a reminder of line metadata types that
        # aren't supported, but were found in the input file
        self.warn_for_unsupported_meta_types()

        for row_input in parsed_row_inputs:
            row_input.fractional_time_digits = self.max_fractional_time_digits

        # after reading all rows in the file, loop over combinatorially-defined columns (as
        # defined by column headers) and remove any from our tracking that were completely empty
        # or that only contain a single value per row. This resolves a boundary condition where
        # the NamingStrategy defines these values as required for computing line names
        # combinatorially, but the lack of combinatorial values allows us to safely ignore the
        # column(s)
        for col_index in column_layout.combinatorial_col_indices:
            meta_values = set()
            multivalued_row = False

            line_mtype_pk = column_layout.col_index_to_line_meta_pk.get(col_index)

            # ignore columns that contain assay metadata, since it's not required for computing
            # line names. TODO: this creates similar boundary conditions for assay meta columns as
            # this code block attempts to resolve for lines (poor detection of empty or completely
            # single-valued columns).  Doing all the analogous introspection, caching,
            # etc for assay-related data is a more comprehensive code change we'll make later on.
            if not line_mtype_pk:
                continue

            for row_input in parsed_row_inputs:
                line_values = row_input.get_related_object_ids(line_mtype_pk)
                meta_values.update(line_values)

                if len(line_values) > 1:
                    multivalued_row = True
                    break

            if not meta_values or not multivalued_row:
                column_layout.combinatorial_col_indices.remove(col_index)

        # having now winnowed down the columns that actually contain combinatorial data,
        # do a proactive check for duplicate input line names if they're predictable (i.e. there's
        # no auto-naming in play due to combinatorially-defined lines).  This should make the
        # workflow more intuitive by catching input errors early and giving a more direct error
        # message that relates exclusively to the user's input.  Also improves efficiency by
        # avoiding potentially expensive ICE queries and preview creation by catching the
        # problem early.
        if column_layout.combinatorial_col_indices:
            line_name_to_rows = defaultdict(list)
            for row in parsed_row_inputs:
                line_name = row.base_line_name
                cell_nums = line_name_to_rows[line_name]
                cell_nums.append(cell_coords(row.row_number - 1, column_layout.line_name_col))

            for line_name, cell_nums in line_name_to_rows.items():
                if len(cell_nums) > 1:
                    cell_nums = ', '.join(cell_nums)
                    message = f'"{line_name}" ({cell_nums})'
                    importer.add_error(INVALID_FILE_VALUE_CATEGORY, DUPLICATE_LINE_NAME_LITERAL,
                                       message)

        # Enforce consistency of ICE folder contents and filtering across all inputs.
        # This allows us to make simplifying assumptions later on when processing ICE folders
        unique_folder_ids = set()
        for index, combo in enumerate(parsed_row_inputs):

            input_folders = combo.get_related_object_ids(ICE_FOLDERS_KEY)
            if index > 0:
                for folder_id in input_folders:
                    if folder_id not in unique_folder_ids:
                        importer.add_error(INVALID_FILE_VALUE_CATEGORY, INCONSISTENT_FOLDERS,
                                           f'{folder_id} (row {combo.row_number})')
                    else:
                        current_filters = combo.folder_id_to_folders[folder_id]
                        if set(current_filters) != set(self.folder_id_to_filters[folder_id]):
                            importer.add_error(INVALID_FILE_VALUE_CATEGORY, INCONSISTENT_FILTERS,
                                               f'row {combo.row_number}')

        return parsed_row_inputs

    def warn_for_unsupported_meta_types(self):
        column_layout = self.column_layout
        importer = self.importer

        unsupported_value_columns = [
            col_index
            for col_index, meta_pk in column_layout.col_index_to_line_meta_pk.items()
            if meta_pk in self.unsupported_line_meta_types_by_pk
        ]
        if unsupported_value_columns:
            line_meta_types = self.cache.line_meta_types
            unsupported_values = []
            for col_index in unsupported_value_columns:
                meta_pk = column_layout.get_line_meta_pk(col_index)
                meta_type = line_meta_types[meta_pk]
                col_letter = get_column_letter(col_index + 1)
                unsupported_values.append(f'"{meta_type.type_name}" (col {col_letter})')

            importer.add_warning(IGNORED_INPUT_CATEGORY, UNSUPPORTED_LINE_METADATA,
                                 ', '.join(unsupported_values))

    def _parse_column_layout(self, row):
        """
        Detects the layout of a template file by matching cell contents of a row containing the
        minimal required column headers, then comparing additional headers in that row against line
        and assay metadata names in EDD's database. If required column headers aren't found in this
        row, then it is ignored. Columns can be provided in any order.
        :param row: the row to inspect for column headers
        :return: the column layout if required columns were found, or None otherwise
        """
        layout = ColumnLayout(self.importer)

        ###########################################################################################
        # loop over columns in the current row, looking for labels that identify at least the
        # minimum set of required columns
        ###########################################################################################
        for col_index, cell in enumerate(row):
            cell_content = self._raw_cell_value(cell)

            # ignore non-string cells since they can't be the column headers we're looking for
            if not isinstance(cell_content, string_types):
                continue

            std_content = _standardize_label(cell_content)

            # skip this cell if it has no non-whitespace content
            if not std_content:
                continue

            #######################################################################################
            # check whether column label matches one of the fixed labels specified by the file
            # format, but where the column labels don't match a defined line MetadataType
            #######################################################################################
            duplicate_attr_col = None
            if _LINE_NAME_COL_PATTERN.match(std_content):
                duplicate_attr_col = layout.line_name_col is not None
                if not duplicate_attr_col:
                    layout.line_name_col = col_index
            elif _LINE_DESCRIPTION_COL_PATTERN.match(std_content):
                duplicate_attr_col = layout.line_description_col is not None
                if not duplicate_attr_col:
                    layout.line_description_col = col_index
            elif _STRAIN_IDS_SINGULAR_COL_PATTERN.match(std_content):
                layout.strain_ids_col = col_index
                layout.register_line_meta_col(col_index, self.cache.strains_mtype, False)
            elif _STRAIN_IDS_PLURAL_COL_PATTERN.match(std_content):
                layout.strain_ids_col = col_index
                layout.register_line_meta_col(col_index, self.cache.strains_mtype, True)
            elif _REPLICATE_COUNT_COL_PATTERN.match(std_content):
                duplicate_attr_col = layout.replicate_count_col is not None
                if not duplicate_attr_col:
                    layout.replicate_count_col = col_index

            # check whether the column label matches custom data defined in the database
            else:

                # test whether this column is protocol-prefixed assay metadata
                assay_meta_type = self._parse_assay_metadata_header(layout,
                                                                    std_content,
                                                                    cell_content,
                                                                    col_index)

                # if we found the type of this column, proceed to the next
                if assay_meta_type:
                    continue

                # if this column isn't protocol-prefixed, test whether it's for line metadata
                line_metadata_type = self._parse_line_metadata_header(layout,
                                                                      std_content,
                                                                      col_index)

                # if we couldn't process this column, track a warning that describes
                # dropped columns (can be displayed later in the UI)
                if not line_metadata_type:
                    cell = header_desc(cell_content, col_index)
                    logger.warning(f'Bad column header {cell}')
                    self.importer.add_error(BAD_FILE_CATEGORY, INVALID_COLUMN_HEADER, cell)

            if duplicate_attr_col:
                self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_LINE_ATTR,
                                        header_desc(cell_content, col_index))

        # return the columns found in this row if at least the
        # minimum required columns were found
        found_col_labels = layout.line_name_col is not None
        if found_col_labels:
            return layout

        return None

    def _raw_cell_value(self, cell):
        if self._is_excel:
            return cell.value
        return cell

    def _parse_assay_metadata_header(self, layout, upper_content, original_content, col_index):
        """
        :return: a truthy value if the content should be treated as assay metadata (the
            MetadataType if one was found, or True if it was clearly intended to be one, but was
            logged as an error).
        """

        ########################################################################################
        # loop over protocol names, testing for a protocol prefix in the column header
        ########################################################################################
        for upper_protocol_name, protocol in self.protocols_by_name.items():
            if upper_content.startswith(upper_protocol_name):

                # pull out the column header suffix following the protocol.
                # it should match the name of an assay metadata type
                start_index = len(upper_protocol_name)
                assay_meta_suffix = upper_content[start_index:].strip()

                suffix_meta_type = None
                is_combinatorial = False

                ################################################################################
                # loop over assay metadata types, testing for an assay metadata suffix in the
                # column header
                ################################################################################
                assay_items = self.assay_metadata_types_by_name.items()
                for upper_type_name, assay_metadata_type in assay_items:

                    # if this type has units, check whether column header matches the type name
                    # with an optional unit suffix
                    if assay_metadata_type.postfix:
                        singular_regex = _TYPE_NAME_REGEX % {
                            'type_name': upper_type_name,
                            'units': assay_metadata_type.postfix
                        }
                        suffix_meta_type = assay_metadata_type if re.match(
                                singular_regex, assay_meta_suffix, re.IGNORECASE) else None
                    # otherwise, check whether the column header exactly matches the type name
                    # (case-insensitive)
                    else:
                        # look for an exact match
                        suffix_meta_type = (assay_metadata_type
                                            if assay_meta_suffix == upper_type_name else None)

                    # if we've found the assay metadata type for this column, stop looking
                    if suffix_meta_type is not None:
                        break

                    # if the column header didn't match the assay metadata type in its
                    # raw form, look for a pluralized version of the metadata
                    # type name. Pluralization indicates the contents should be treated as a
                    # comma-delimited list of combinatorial metadata values
                    meta_regex = _PLURALIZED_REGEX % {
                        'type_name': re.escape(assay_metadata_type.type_name),
                        'units': re.escape(assay_metadata_type.postfix)
                    }
                    pluralized_match = re.match(meta_regex, assay_meta_suffix, re.IGNORECASE)

                    if pluralized_match:
                        is_combinatorial = True
                        logger.debug(f'Column header suffix {assay_meta_suffix} matched '
                                     f'pluralized regex {meta_regex}')
                        suffix_meta_type = assay_metadata_type
                        break
                    else:
                        logger.debug(f'Column header suffix {assay_meta_suffix} did not match '
                                     f'pluralized regex {meta_regex}')

                # if the column started with the name of a protocol and ended with an
                # assay metadata type name, store the association of this column with the
                # (Protocol, MetadataType) combination
                if suffix_meta_type:
                    layout.register_assay_meta_column(original_content, col_index,
                                                      upper_protocol_name,
                                                      protocol, suffix_meta_type,
                                                      is_combinatorial)
                    return suffix_meta_type

                # otherwise, since the column header was prefixed with a valid
                # protocol name, assume there was a data entry error or missing metadata type
                # in the database. This check is especially important for the Time metadata
                # assumed by the file format.
                else:
                    orig_case_suffix = original_content[start_index:].strip()
                    value = header_desc(orig_case_suffix, col_index)
                    logger.debug(f"Column header suffix {value} didn't match known metadata types")
                    self.importer.add_error(BAD_FILE_CATEGORY, UNMATCHED_ASSAY_COL_HEADERS_KEY,
                                            value)
                    return True

    def _parse_line_metadata_header(self, column_layout, std_cell_content, col_index):
        """
        :return: the line MetadataType if one was found or None otherwise
        """
        result = None

        # if we didn't find the singular form of the column header as line metadata, look
        # for a pluralized version that we'll treat as combinatorial line creation input
        for std_type_name, meta_type in self.line_mtypes_by_name.items():

            # check whether column header matches the type name with an optional unit suffix
            if meta_type.postfix:
                singular_regex = _TYPE_NAME_REGEX % {
                    'type_name': std_type_name, 'units': meta_type.postfix
                }
                result = (meta_type if re.match(singular_regex, std_cell_content, re.IGNORECASE)
                          else None)
            # otherwise, check whether the column header exactly matches the type name
            # (case-insensitive, whitespace collapsed in both)
            else:
                # look for an exact match
                result = meta_type if std_cell_content == std_type_name else None

            if result is not None:
                column_layout.register_line_meta_col(col_index, result)
                return result

            # if we didn't find a singular version of the column header, check for a pluralized
            # version, which indicates that cell values should be treated as combinatorial input
            meta_regex = _PLURALIZED_REGEX % {
                'type_name': re.escape(meta_type.type_name),
                'units': re.escape(meta_type.postfix)
            }
            pluralized_match = re.match(meta_regex, std_cell_content, re.IGNORECASE)

            if pluralized_match:
                result = meta_type
                column_layout.register_line_meta_col(col_index, result, is_combinatorial=True)
                logger.debug(f'Column header "{std_cell_content}" matches line metadata type '
                             f'"{std_type_name}"')
                return result

        return None

    def _parse_row(self, row, row_index):
        """
        Reads a single spreadsheet row to find line creation inputs. The row is read even if errors
        occur, logging errors in the 'errors' parameter so that multiple user input errors can be
        detected and communicated during a single pass of editing the file.
        """
        row_inputs = _ExperimentDescriptionFileRow(self.column_layout, self.cache, row_index+1,
                                                   self.importer)

        # parse line name -- dictates whether the row is valid
        abort_row = not self._parse_line_name(row, row_inputs, row_index)
        if abort_row:
            return None

        # parse other optional line attributes or special-case columns
        self._parse_description(row_inputs, row, row_index)
        self._parse_control(row_inputs, row, row_index)
        self._parse_replicates(row_inputs, row, row_index)

        # parse line metadata
        self._parse_line_meta(row_inputs, row, row_index)

        # parse assay metadata, e.g. proteomics time
        self._parse_assay_metadata(row_inputs, row, row_index)

        return row_inputs

    def _parse_line_name(self, row, row_inputs, row_index):
        """
        Parses line name from a file row, logging errors if needed
        """
        str_cell_content = self._get_str_cell_content(
            row,
            row_index,
            self.column_layout.line_name_col,
            convert_to_string=True
        )

        if str_cell_content:
            row_inputs.base_line_name = str_cell_content
            return True

        # detect whether the row is completely empty...if not, raise an error. Note that a
        # completely empty row is the only case where we can safely ignore the missing
        # required value.
        for col_index, cell in enumerate(row):
            cell_val = self._raw_cell_value(cell)
            if cell_val is not None and str(cell_val).strip():
                name_cell_num = cell_coords(row_index, self.column_layout.line_name_col)
                logger.info(f'Parse error: Cell {name_cell_num} was empty, but was expected '
                            'to contain a name for the EDD line')
                self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                        MISSING_REQUIRED_LINE_NAME,
                                        name_cell_num)
                return False

        logger.info(f'Ignored empty row {row_index + 1}.')
        return False

    def _parse_description(self, row_inputs, row, row_index):
        desc_col = self.column_layout.line_description_col
        if desc_col is None:
            return

        str_cell_content = self._get_str_cell_content(
            row,
            row_index,
            desc_col
        )
        if str_cell_content:
            row_inputs.description = str_cell_content

    def _parse_control(self, row_inputs, row, row_index):
        ctrl_col = self.column_layout.line_control_col
        if ctrl_col is None:
            return

        str_cell_content = self._get_str_cell_content(
            row,
            row_index,
            ctrl_col
        )

        if str_cell_content:
            tokens = str_cell_content.split(',')
            if len(tokens) == 1:
                tokens = str_cell_content

            values = []
            for token in tokens:
                is_control = "TRUE" == token or "YES" == str_cell_content
                values.append(is_control)

            if len(values) > 1:
                pk = self.ctrl_meta_type.pk
                row_inputs.combinatorial_line_metadata[pk] = values

    def _parse_replicates(self, row_inputs, row, row_index):
        replicate_col = self.column_layout.replicate_count_col
        if replicate_col is None:
            return

        raw_val = self._raw_cell_value(row[replicate_col])

        if raw_val is None:
            row_inputs.replicate_count = 1
            self.importer.add_warning(IGNORED_INPUT_CATEGORY,
                                      ROWS_MISSING_REPLICATE_COUNT,
                                      row_index+1)
            return

        try:
            row_inputs.replicate_count = int(raw_val)
            if row_inputs.replicate_count == 0:
                cell = cell_coords(row_index, replicate_col)
                self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, ZERO_REPLICATES, cell)
        except ValueError:
            cell = cell_desc(raw_val, row_index, replicate_col)
            self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, INVALID_REPLICATE_COUNT,
                                    cell)

    def _parse_line_meta(self, row_inputs, row, row_index):
        col_to_meta_pk = self.column_layout.col_index_to_line_meta_pk
        if not col_to_meta_pk:
            return

        for col_index, mtype_pk in col_to_meta_pk.items():
            # skip values for metadata types we've specifically disabled (with a warning)
            # earlier in the parsing process
            if mtype_pk in self.unsupported_line_meta_types_by_pk:
                continue

            str_cell_content = self._get_str_cell_content(
                row,
                row_index,
                col_index,
                convert_to_string=True
            )

            if not str_cell_content:
                continue

            ###################################################
            # Strain part id(s)
            ###################################################
            # TODO: after some initial testing, consider adding custom group support for
            #  Carbon Source similar to that for strains. Since some changes to Carbon
            #  Source / Media tracking are needed, we will probably want to defer support
            # for Carbon Sources for now.
            if col_index == self.column_layout.strain_ids_col:
                self.parse_strains_cell(row, row_index, row_inputs)
                continue

            if col_index in self.column_layout.combinatorial_col_indices:
                self._parse_combinatorial_input(
                    row_inputs,
                    str_cell_content,
                    row_index,
                    col_index,
                    mtype_pk,
                    INVALID_FILE_VALUE_CATEGORY,
                    UNPARSEABLE_COMBINATORIAL_VALUE,
                )
            else:
                row_inputs.add_common_line_metadata(mtype_pk, str_cell_content)

    def parse_strains_cell(self, cols_list, row_index, row_inputs):
        layout = self.column_layout
        strain_ids_col = layout.strain_ids_col
        col_combinatorial = strain_ids_col in layout.combinatorial_col_indices
        cell_content = self._get_str_cell_content(cols_list, row_index, strain_ids_col,
                                                  convert_to_string=True)

        combinatorial_strain_id_groups = []  # list of lists of strain IDs

        # individual part ID's that were read externally to strain groups (may be present in
        # either combinatorial or  non-combinatorial use)
        individual_strain_ids = []

        if not cell_content:
            return

        # Extract comma-delimited tokens from the cell, for fault tolerance, casting to string in
        # case user entered something else (e.g. long).
        #
        # Each valid token should be EITHER:
        # A) An ICE part ID, or
        # B) A strain group: A paren-enclosed, semicolon-delimited list of ICE part ID's. A strain
        #    group allows multiple strains to be assigned to the same line, or for lines to be
        #    created combinatorially from multiple strain groups, depending on whether the column
        #    header defines the column as allowing combinatorial entry
        tokens = str(cell_content).split(',')

        # loop over comma-delimited tokens included in the cell
        for token in tokens:
            token = token.strip()

            # ignore whitespace and multiple commas in a row
            if not token:
                continue

            # if this token is a paren-enclosed list of part numbers, it's a
            # strain group rather than single strain to be included in the list.
            token_strain_grp_match = _STRAIN_GROUP_PATTERN.match(token)

            cell_num = cell_coords(row_index, strain_ids_col)
            token_description = f'"{token}" ({cell_num})'

            # if token matches our regex for a strain ID group
            if token_strain_grp_match:
                grp = token_strain_grp_match.group(1)
                logger.debug(f'Token "{token}" is a strain group. Match subgroup 1: "{grp}"')
                strain_group = [strain_id.strip() for strain_id in
                                token_strain_grp_match.group(1).split(_STRAIN_GROUP_MEMBER_DELIM)]

                for strain_id in strain_group:
                    self._check_part_id_pattern(strain_id, cell_num)

                if col_combinatorial:
                    combinatorial_strain_id_groups.append(strain_group)
                else:
                    # if the column header indicates non-combinatorial use and cell contains a
                    # strain group plus any other value, then the content is badly formatted. A
                    # non-combinatorial column should only contain a single strain or list of
                    # strains. As likely as not, this is bad user input, so we'll treat it as an
                    #  error.
                    if len(tokens) > 1:
                        self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                                INCONSISTENT_COMBINATORIAL_VALUE,
                                                token_description)
                    # tolerate strain group syntax for a non-combinatorially defined part id
                    # column, since the list of included strains is equally valid whether
                    # comma-delimited or enclosed in parens and semicolon-delimited (e.g as pasted
                    # from another sheet or row)
                    else:
                        row_inputs.combinatorial_strain_id_groups.append(strain_group)

            # value isn't a valid strain group...should be an ICE part number
            else:
                logger.debug(f'Token "{token}" is NOT a strain group')
                if _STRAIN_GROUP_MEMBER_DELIM not in token:
                    individual_strain_ids.append(token)
                    self._check_part_id_pattern(token, cell_num)
                else:
                    self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                            DELIMETER_NOT_ALLOWED_VALUE, token_description)

        logger.debug(f'Done parsing strains for row {row_index + 1}. Individual strains: '
                     f'{individual_strain_ids}, combo_groups: {combinatorial_strain_id_groups}')

        # depending on whether tho column header defines combinatorial input, either submit
        # comma-delimited strains as a single group (co-culture), or for combinatorial line
        # creation
        strains_pk = self.cache.strains_mtype.pk
        if col_combinatorial:
            if individual_strain_ids:
                # tolerate any single part numbers included among strain groups by treating
                # each as its own strain id group
                for part_id in individual_strain_ids:
                    combinatorial_strain_id_groups.append([part_id])

            if combinatorial_strain_id_groups:
                row_inputs.combinatorial_line_metadata[strains_pk] = combinatorial_strain_id_groups

        else:
            if individual_strain_ids:
                row_inputs.common_line_metadata[strains_pk] = individual_strain_ids
            elif combinatorial_strain_id_groups:
                row_inputs.common_line_metadata[strains_pk] = combinatorial_strain_id_groups

    def _check_part_id_pattern(self, part_id, input_location=''):
        # test whether the strain's part number matched the expected pattern.
        # we'll allow all input through to the ICE query later in case our pattern
        # is dated, but this way we can provide a more helpful prompt for bad
        # user input
        part_number_match = TYPICAL_ICE_PART_NUMBER_PATTERN.match(part_id)

        if not part_number_match:
            opt_location_str = f' ({input_location})' if input_location else ''
            desc = f'"{part_id}"{opt_location_str}'
            self.importer.add_warning(POSSIBLE_USER_ERROR_CATEGORY,
                                      PART_NUMBER_PATTERN_UNMATCHED_WARNING,
                                      desc)
            logger.warning(f'Expected ICE part number(s) but {desc}, did not match the expected '
                           f'pattern. This is either bad user input, or indicates that the '
                           f'pattern needs updating.')

    def _parse_assay_metadata(self, row_inputs, row, row_index):
        layout = self.column_layout
        if not layout.col_index_to_assay_data:
            return

        # loop over per-protocol assay metadata columns
        assay_mtype_cols = layout.col_index_to_assay_data
        for col_index, (protocol, assay_mtype) in assay_mtype_cols.items():

            str_cell_content = self._get_str_cell_content(
                row,
                row_index,
                col_index, convert_to_string=True
            )

            # skip blank cells
            if not str_cell_content:
                continue

            is_time = assay_mtype.pk == self.cache.assay_time_mtype.pk
            err_category = INVALID_FILE_VALUE_CATEGORY
            error_key = (INCORRECT_TIME_FORMAT if is_time else
                         UNPARSEABLE_COMBINATORIAL_VALUE)

            if col_index not in layout.combinatorial_col_indices:
                cell_content = str_cell_content

                # do some additional processing of time values to detect bad user entries for time.
                # best to catch errors here since time's currently stored in hstore as a string.
                # We should eventually extend similar support to other metadata types defined as
                # numeric.
                if is_time:
                    try:
                        cell_content = self._time_parser.parse(str_cell_content)
                    except ValueError as v:
                        bad_value = cell_desc(str_cell_content, row_index, col_index)
                        logger.exception(f'ValueError parsing {bad_value}')
                        self.importer.add_error(err_category, error_key, bad_value)

                row_inputs.add_common_assay_metadata(
                    protocol.pk,
                    assay_mtype.pk,
                    cell_content
                )
                continue

            # if this cell is in a column of combinatorial input, add it to that list
            parser = self._time_parser if is_time else _RAW_STRING_PARSER

            self._parse_combinatorial_input(
                row_inputs,
                str_cell_content,
                row_index,
                col_index,
                assay_mtype.pk,
                err_category,
                error_key,
                protocol,
                parser
            )

    def _get_str_cell_content(self, row, row_index, col_index, convert_to_string=False):
        """
        Reads cell content and converts it to a string for simplified storage in Postgres hstore.
        TODO: this method should be updated when we transition to using a JSON field instead.
        :param convert_to_string: True to convert the cell value to a string if it isn't stored
        as one.
        """
        cell_content = self._raw_cell_value(row[col_index])

        if cell_content is None:
            return None

        if isinstance(cell_content, string_types):
            return cell_content.strip()

        else:
            actual_type = type(cell_content).__name__
            if convert_to_string:
                return str(cell_content)
            else:
                col_letter = get_column_letter(col_index+1)
                msg = f'{row_index+1}{col_letter} (value: {cell_content}, type: {actual_type})'
                self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, INVALID_CELL_TYPE, msg)
                return None

    # TODO: lots of refactoring to do here!
    def _parse_combinatorial_input(self, row_inputs, cell_content, row_index, col_index,
                                   mtype_pk, error_category, error_key, protocol=None,
                                   value_parser=_RAW_STRING_PARSER):
        """
        Parses the value of a single cell that may / may not have comma-separated combinatorial
        content.
        """

        for token in cell_content.split(','):
            token = token.strip()
            try:
                parsed_val = value_parser.parse(token)

                if protocol:
                    row_inputs.add_combinatorial_assay_metadata(protocol.pk, mtype_pk, parsed_val)
                else:
                    row_inputs.add_combinatorial_line_metadata(mtype_pk, parsed_val)

            except ValueError:
                cell_num = cell_coords(row_index, col_index)
                bad_value = f'"{token}" ({cell_num})'
                logger.exception(f'ValueError parsing token {token}')
                self.importer.add_error(error_category, error_key, bad_value)
                break


class JsonInputParser(CombinatorialInputParser):
    """
    Parses / verifies JSON input for combinatorial line creation. Note that instances of this
    class maintain a cache of protocols and metadata types defined in the database, so the
    lifecycle of each instance should be short.
    """
    def __init__(self, cache, importer=None):
        super(JsonInputParser, self).__init__(cache, importer)
        self.max_fractional_time_digits = 0
        self.parsed_json = None

    def parse(self, stream):

        combinatorial_inputs = []
        importer = self.aggregator

        if importer.errors:
            return None

        ###########################################################################################
        # Once validated, parse the JSON string into a Python dict or list
        # if validation succeeded, extract the naming strategy from the JSON, then pass the rest
        # as parameters to CombinatorialDescriptionInput. Also cache for possible later use by
        # client code (e.g. in err emails)

        parsed_json = json.loads(stream)
        self.parsed_json = parsed_json

        if not parsed_json:
            return None

        validator = Draft4Validator(JSON_SCHEMA)
        validation_errors = validator.iter_errors(parsed_json)
        for err in validation_errors:
            path_str = ('.'.join(str(elt) for elt in err.relative_path) if err.relative_path else
                        'root')
            importer.add_error(
                BAD_GENERIC_INPUT_CATEGORY, INVALID_JSON,
                f'{path_str}: {err.message}'
            )
        if importer.errors:
            logger.error(f'Aborting parsing due to JSON validation errors: {validation_errors}')
            return None

        max_decimal_digits = 0

        # tolerate either a sequence of CombinatorialDescriptionInput or a single one
        if not isinstance(parsed_json, Sequence):
            parsed_json = [parsed_json]

        for value in parsed_json:

            # work around reserved 'description' keyword in JSON schema
            description = value.pop('desc', None)

            # convert string-based keys required by JSON into their numeric equivalents to simplify
            # primary key dict lookups later on
            common_line_metadata = _copy_to_numeric_keys(value.pop(COMMON_LINE_METADATA_SECTION,
                                                                   {}))
            combinatorial_line_metadata = _copy_to_numeric_keys(
                    value.pop(COMBINATORIAL_LINE_METADATA_SECTION, {}))
            protocol_to_assay_metadata = _copy_to_numeric_keys(
                    value.pop(PROTOCOL_TO_ASSAY_METADATA_SECTION, {}))
            protocol_to_combinatorial_metadata = _copy_to_numeric_keys(
                    value.pop(PROTOCOL_TO_COMBINATORIAL_METADATA_SECTION, {}))
            ice_folder_to_filters = _copy_to_numeric_keys(value.pop('ice_folder_to_filters', {}))
            custom_name_elts = value.pop('custom_name_elts', {})

            naming_elements = value.pop(NAME_ELEMENTS_SECTION, None)
            if naming_elements:
                elements = _copy_to_numeric_elts(naming_elements[ELEMENTS_SECTION])
                abbreviations = _process_abbrev_keys(naming_elements.get(ABBREVIATIONS_SECTION,
                                                                         {}))
                naming_strategy = AutomatedNamingStrategy(importer, self.cache,
                                                          naming_elts=elements,
                                                          custom_name_elts=custom_name_elts,
                                                          abbreviations=abbreviations)
            else:
                base_name = value.pop(BASE_NAME_ELT)
                naming_strategy = _ExperimentDescNamingStrategy(ColumnLayout(self), self.cache)
                naming_strategy.base_line_name = base_name

            try:
                # just pass the JSON as initializer arguments. Won't verify the internal
                # structure/expected data types, but for starters that's probably a safe bet
                combo_input = CombinatorialDescriptionInput(
                    naming_strategy,
                    importer,
                    description=description,
                    common_line_metadata=common_line_metadata,
                    combinatorial_line_metadata=combinatorial_line_metadata,
                    protocol_to_assay_metadata=protocol_to_assay_metadata,
                    protocol_to_combinatorial_metadata=protocol_to_combinatorial_metadata,
                    ice_folder_to_filters=ice_folder_to_filters)
                combo_input.replicate_count = value.get(REPLICATE_COUNT_ELT, 1)

                # inspect JSON input to find the maximum number of decimal digits in the user input
                assay_time_pk = self.cache.assay_time_mtype.pk
                if assay_time_pk:
                    for protocol, assay_metadata in protocol_to_assay_metadata.items():
                        time_values = assay_metadata.get(assay_time_pk, [])
                        for time_value in time_values:
                            str_value = str(time_value)
                            if str_value != str((int(float(time_value)))):
                                decimal_digits = len(str_value) - str_value.find('.') - 1
                                max_decimal_digits = max(max_decimal_digits, decimal_digits)

                naming_strategy.combinatorial_input = combo_input
                combinatorial_inputs.append(combo_input)
            except RuntimeError as rte:
                logger.exception('Unexpected parse error')
                importer.add_error(INTERNAL_EDD_ERROR_CATEGORY, PARSE_ERROR, str(rte))

        # verify primary key inputs from the JSON are for the expected MetaDataType context,
        # and that they exist, since there's no runtime checking for this at database item
        # creation time when they get (necessarily) shoved into the hstore field. Note that this is
        # an non-ideal comparison of database fields cached in memory, but should be an acceptable
        # risk of having a stale cache of these values, which should be recently gathered and very
        # unlikely to be stale.

        for combo_input in combinatorial_inputs:
            combo_input.verify_pks(self.cache, importer)

        # consistently use decimal or integer time in assay names based on whether any fractional
        # input was provided
        for combo_input in combinatorial_inputs:
            combo_input.fractional_time_digits = max_decimal_digits

        return combinatorial_inputs


def _copy_to_numeric_elts(input_list):
    converted_list = []
    for index, element in enumerate(input_list):
        try:
            int_value = int(element)
            converted_list.append(int_value)
        except ValueError:
            converted_list.append(element)

    return converted_list


def _copy_to_numeric_keys(input_dict):
    converted_dict = {}
    for key, value in input_dict.items():

        # if value is a nested dict, do the same work on it
        if isinstance(value, dict):
            value = _copy_to_numeric_keys(value)
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict


def _process_abbrev_keys(input_dict):
    """
    Replaces only first-order keys with numbers for cases where they match.  This simplifies
    comparison with metadata values during line naming.  Metadata values are are always stored
    as strings, even if they're actually numeric.
    """
    converted_dict = {}
    for key, value in input_dict.items():
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict
