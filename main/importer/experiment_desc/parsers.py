# coding: utf-8

import json
import logging
import re

from collections import Sequence
from jsonschema import Draft4Validator
from openpyxl.utils.cell import get_column_letter
from six import string_types

from .constants import (
    ABBREVIATIONS_SECTION,
    BAD_FILE_CATEGORY,
    BAD_GENERIC_INPUT_CATEGORY,
    BASE_NAME_ELT,
    COMBINATORIAL_LINE_METADATA_SECTION,
    COMMON_LINE_METADATA_SECTION,
    DELIMETER_NOT_ALLOWED_VALUE,
    DUPLICATE_ASSAY_METADATA,
    ELEMENTS_SECTION,
    EMPTY_WORKBOOK,
    ICE_FOLDERS_KEY,
    IGNORED_INPUT_CATEGORY,
    INCONSISTENT_FILTERS,
    INCONSISTENT_FOLDERS,
    INCONSISTENT_COMBINATORIAL_VALUE,
    INCORRECT_TIME_FORMAT,
    INTERNAL_EDD_ERROR_CATEGORY,
    INVALID_CELL_TYPE,
    INVALID_COLUMN_HEADER,
    INVALID_FILE_VALUE_CATEGORY,
    INVALID_JSON,
    INVALID_REPLICATE_COUNT,
    MISSING_REQUIRED_LINE_NAME,
    MULTIPLE_WORKSHEETS_FOUND,
    NAME_ELEMENTS_SECTION,
    PARSE_ERROR,
    PART_NUMBER_PATTERN_UNMATCHED_WARNING,
    POSSIBLE_USER_ERROR_CATEGORY,
    PROTOCOL_TO_ASSAY_METADATA_SECTION,
    PROTOCOL_TO_COMBINATORIAL_METADATA_SECTION,
    REPLICATE_COUNT_ELT,
    ROWS_MISSING_REPLICATE_COUNT,
    UNMATCHED_ASSAY_COL_HEADERS_KEY,
    UNPARSEABLE_COMBINATORIAL_VALUE,
    UNSUPPORTED_LINE_METADATA,
    ZERO_REPLICATES,
)
from .utilities import (
    ALLOWED_RELATED_OBJECT_FIELDS,
    AutomatedNamingStrategy,
    CombinatorialDescriptionInput,
    NamingStrategy
)
from jbei.utils import TYPICAL_JBEI_ICE_PART_NUMBER_REGEX
from main.importer.experiment_desc.validators import SCHEMA as JSON_SCHEMA


logger = logging.getLogger(__name__)

TYPICAL_ICE_PART_NUMBER_PATTERN = re.compile(TYPICAL_JBEI_ICE_PART_NUMBER_REGEX, re.IGNORECASE)

###################################################################################################
# Column header patterns for the experiment description file.
# These match values generated by the ICE bulk export and also consumed by EDD's bulk line creation
# script, so don't change them arbitrarily!
LINE_NAME_COL_LABEL = 'Line\s+Name'
LINE_DESCRIPTION_COL_REGEX = 'Line\s+Description'
STRAIN_IDS_COL_LABEL = 'Part\s+ID'
REPLICATE_COUNT_COL_REGEX = 'Replicate\s+Count'

_LINE_NAME_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_NAME_COL_LABEL, re.IGNORECASE)
_LINE_DESCRIPTION_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_DESCRIPTION_COL_REGEX,
                                           re.IGNORECASE)
_STRAIN_IDS_SINGULAR_COL_PATTERN = re.compile(r'^\s*%s\s*$' % STRAIN_IDS_COL_LABEL, re.IGNORECASE)
_STRAIN_IDS_PLURAL_COL_PATTERN = re.compile(r'^\s*%s\s*\(s\)$' % STRAIN_IDS_COL_LABEL,
                                            re.IGNORECASE)
_REPLICATE_COUNT_COL_PATTERN = re.compile(r'^\s*%s\s*$' % REPLICATE_COUNT_COL_REGEX, re.IGNORECASE)
###################################################################################################

_STRAIN_GROUP_MEMBER_DELIM = ';'
_STRAIN_GROUP_REGEX = r'^\s*\(((?:\s*[^' + _STRAIN_GROUP_MEMBER_DELIM + '\)\(]+\s*' + \
                      _STRAIN_GROUP_MEMBER_DELIM + '?\s*)+)\)\s*$'
_STRAIN_GROUP_PATTERN = re.compile(_STRAIN_GROUP_REGEX)

_TIME_VALUE_REGEX = r'^\s*(\d+(?:\.\d+)?)\s*h\s*$'
_TIME_VALUE_PATTERN = re.compile(_TIME_VALUE_REGEX, re.IGNORECASE)

# tests whether the input string ends with 's' or '(s)'
_OPT_UNIT_SUFFIX = r'(?:\s*(?:\(%(units)s\)|%(units)s))?'
_TYPE_NAME_REGEX = r'^%(type_name)s' + _OPT_UNIT_SUFFIX + '$'
_PLURALIZED_REGEX = r'^%(type_name)s(?:S|\(S\))' + _OPT_UNIT_SUFFIX + '$'

_WHITESPACE_PATTERN = re.compile(r'\s+')


class _AssayMetadataValueParser(object):
    def parse(self, raw_value_str):
        """
        Parses the raw string input for a single assay metadata value
        :return the parsed value to store, or None if no value should be stored
        :raise ValueError if the value couldn't be parsed
        """
        raise ValueError("Choose one of the specific sub-classes to parse value")


class _RawStringValueParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        stripped = raw_value_str.strip()
        if not stripped:
            return None
        return stripped


class _DecimalTimeParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        match = _TIME_VALUE_PATTERN.match(raw_value_str)
        if match:
            str_value = match.group(1)
            number_value = float(str_value)  # raises ValueError as in the spec
            stripped = str(str_value.strip()).replace(',', '').replace('-', '').replace('+', '')
            sep_index = stripped.find('.')  # TODO: i18n

            fractional_digits = 0
            if sep_index >= 0:
                fractional_digits = (len(stripped) - sep_index) - 1  # TODO: commas!
            return number_value, fractional_digits
        raise ValueError(
            'Value "%s" did not match the expected time pattern (e.g. "4.0h")' % raw_value_str
        )


# stateless value parsing strategies for metadata input (time is treated specially by the file
# format)
RAW_STRING_PARSER = _RawStringValueParser()
TIME_PARSER = _DecimalTimeParser()


class ColumnLayout:
    """
    Stores column layout read from the header row of an experiment description file. Since these
    files are designed to be user edited, parsing should be as tolerant as possible.
    """

    def __init__(self, importer):
        self.line_name_col = None
        self.line_description_col = None
        self.line_control_col = None
        self.replicate_count_col = None
        self.strain_ids_col = None
        self.col_index_to_line_meta_pk = {}
        self.col_index_to_assay_data = {}  # maps col index -> (Protocol, MetadataType)

        # indices of all *any* columns for combinatorial creation (both metadata AND strains!)
        self.combinatorial_col_indices = []
        self.unique_assay_protocols = {}
        self.importer = importer

    def register_protocol(self, protocol):
        self.unique_assay_protocols[protocol.pk] = True

    def has_assay_metadata(self, upper_protocol_name, metadata_pk):
        """
        Tests whether any columns have been detected so far that store a specific (Protocol,
        MetadataType) pair.

        :param upper_protocol_name:
        :param metadata_pk:
        :return:
        """
        items = self.col_index_to_assay_data.items()
        for col_index, (existing_protocol, existing_assay_meta_type) in items:
            if ((upper_protocol_name == existing_protocol.name.upper()) and
                    (metadata_pk == existing_assay_meta_type.pk)):
                return True
        return False

    def combinatorial_line_col_order(self):
        return filter(
            lambda x: x in self.col_index_to_line_meta_pk,
            self.combinatorial_col_indices
        )

    def combinatorial_assay_col_order(self):
        return filter(
            lambda x: x in self.col_index_to_assay_data,
            self.combinatorial_col_indices
        )

    def register_assay_meta_column(self, col_index, upper_protocol_name, protocol, assay_meta_type,
                                   is_combinatorial):

        # test for duplicate use of the same (Protocol, MetadataType) combination.
        # if we see it, log an error -- no clear/automated way for us to resolve which column
        #  has the correct values!
        if self.has_assay_metadata(upper_protocol_name, assay_meta_type.pk):
            self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_ASSAY_METADATA,
                                    assay_meta_type.pk)

        self.register_protocol(protocol)

        self.col_index_to_assay_data[col_index] = (protocol, assay_meta_type)
        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)

        logger.debug(
            'Column %(col)d matches protocol "%(protocol)s", assay metadata type "%(meta_type)s"' %
            {
                'col': col_index + 1,
                'protocol': protocol.name,
                'meta_type': assay_meta_type.type_name
            }
        )

    def get_assay_metadata_type(self, col_index):
        value = self.col_index_to_assay_data.get(col_index, None)
        if not value:
            return None

        return value[1]

    def get_line_meta_pk(self, col_index):
        return self.col_index_to_line_meta_pk.get(col_index, None)

    def get_meta_pk(self, col_index):
        """
        Gets the integer primary key of the Line OR Assay MetadataType the specified ED file column
        :param col_index: the column index
        :return: the MetadataType primary key
        """
        pk = self.get_line_meta_pk(col_index)
        if pk:
            return pk
        assay_mtype = self.get_assay_metadata_type(col_index)
        if not assay_mtype:
            return None
        return assay_mtype.pk

    @property
    def unique_protocols(self):
        return set(self.unique_assay_protocols)

    def set_line_metadata_type(self, col_index, line_metadata_type, is_combinatorial=False):
        logger.debug(
            'Column %d matches line metadata type %s' %
            (col_index+1, line_metadata_type.type_name)
        )

        self.col_index_to_line_meta_pk[col_index] = line_metadata_type.pk

        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)


class _ExperimentDescNamingStrategy(NamingStrategy):
    """
    A simple line/assay naming strategy assumed in the experiment description file use case,
    where line names/assay names are created automatically by from a combination of the base
    line name, plus metadata values for any combinatorially-defined columns that are needed to
    make resulting line/assay names unique. Combinatorial metadata values included in the names are
    added in the order that columns were specified in the file.
    Note that this allows for duplicate assay names within different
    protocols, which should be clear in EDD's UI from protocol filtering and unit markings in the
    visualizations.
    """

    def __init__(self, col_layout, cache, importer):
        super(_ExperimentDescNamingStrategy, self).__init__(cache, importer)
        self.col_layout = col_layout
        self.base_line_name = None

    def get_line_name(self, line_metadata, replicate_num):
        """
        Computes the line name, either by using the explicitly-provided name from the file, OR if
        there are combinatorially-defined columns (by appending an 's' or '(s)' to the column
        header), by iterating over combinatorial columns in the order defined by the file,
        then appending combinatorial metadata values to the line name.  Note that if used,
        replicate number is always at the end regardless of column order.
        """
        layout = self.col_layout
        line_metadata_types = self.cache.line_meta_types
        cache = self.cache

        name_elts = []
        included_base_name = False

        # build the name segment for strains needed to make this line name unique (if any)
        # iterate over combinatorial line metadata columns and construct line name in the same
        # order that name-relevant elements were listed in columns in the input file. We have to
        # include values for the combinatorial metadata so that line names will be unique
        for line_metadata_col in layout.combinatorial_line_col_order():
            line_meta_pk = self.col_layout.get_line_meta_pk(line_metadata_col)

            # if the base line name hasn't been included yet, and comes before this metadata
            # element, insert it
            if (not included_base_name) and (layout.line_name_col < line_metadata_col):
                included_base_name = True
                name_elts.append(self.base_line_name.replace(' ', self.space_replacement))

            # do special processing for Line related object fields needed as part of computing
            # the line name
            if line_meta_pk in cache.related_object_mtypes:
                # as a functional stopgap, just hard-code the single supported related field
                # value and assume that's what was provided...worst case we'll just expose
                # the wrong (allowed) value
                meta_type = cache.line_meta_types[line_meta_pk]
                related_obj_field = ALLOWED_RELATED_OBJECT_FIELDS[meta_type.type_field]
                src_detail = get_column_letter(line_metadata_col)
                segment = self.build_related_objects_name_segment(line_metadata,
                                                                  line_meta_pk,
                                                                  related_obj_field,
                                                                  src_detail)
                name_elts.append(segment)
                logger.debug('Built related object name segment "%s"' % meta_type)
                continue

            metadata_value = line_metadata.get(line_meta_pk, None)  # value is optional!

            if not metadata_value:
                # TODO: add a warning that line names won't be consistent
                continue

            line_meta_type = line_metadata_types[line_meta_pk]
            naming_elt = metadata_value
            if line_meta_type.postfix:
                naming_elt += line_meta_type.postfix
            name_elts.append(naming_elt.replace(' ', self.space_replacement))

        # if the base line name still isn't added, add it
        if not included_base_name:
            name_elts.append(self.base_line_name)
            included_base_name = True

        # if creating more than one replicate, build a suffix to show replicate number so that
        # line names are unique. Replicate number should always be last in the line name
        if self.combinatorial_input.replicate_count > 1:
            replicate_suffix = 'R%d' % replicate_num
            name_elts.append(replicate_suffix)

        logger.debug('Building line name from elements: %s' % str(name_elts))

        # if making lines combinatorially based on line metadata, insert the combinatorial values
        # into the line name so that line names will be unique
        name = self.section_separator.join(name_elts)
        return name

    def get_required_naming_meta_pks(self):
        """
        Gets the Line MetadataTypes that represent categories of data required for computing
        line names during the automated line creation process.  E.g. any data specified
        combinatorially.
        :return: set of integer primary keys for Line MetadataTypes
        """
        col_layout = self.col_layout
        meta_pks = set()

        for col in col_layout.combinatorial_col_indices:
            line_meta_pk = col_layout.col_index_to_line_meta_pk.get(col)
            if line_meta_pk:
                meta_pks.add(line_meta_pk)
        return meta_pks

    def _get_time_format_string(self):
        if self.fractional_time_digits:
            return '%0.' + ('%d' % self.fractional_time_digits + 'f')
        return '%d'

    def get_assay_name(self, line, protocol_pk, assay_metadata):
        layout = self.col_layout
        name_elts = [line.name]
        assay_time_mtype_pk = self.cache.assay_time_mtype.pk

        logger.debug(
            'Combinatorial assay column order: %s' % ','.join(
                    [str(index) for index in layout.combinatorial_assay_col_order()]))

        try:

            # iterate over combinatorial assay metadata columns and construct assay name in the
            # same order that name-relevant elements were listed in columns in the input file.
            # We have to include values for the combinatorial metadata so that assay names will
            # be unique
            for assay_metadata_col in layout.combinatorial_assay_col_order():
                col_protocol, assay_meta_type = layout.col_index_to_assay_data[assay_metadata_col]

                # if this column contained metadata for a different protocol, skip it
                if col_protocol.pk != protocol_pk:
                    continue

                logger.debug(
                    'Inspecting combinatorial assay metadata column %(col)s for protocol '
                    '"%(protocol)s", meta "%(meta_type)s"' % {
                        'col': get_column_letter(assay_metadata_col + 1),
                        'protocol': col_protocol.name,
                        'meta_type': assay_meta_type.type_name})

                metadata_value = assay_metadata.get(assay_meta_type.pk)

                # TODO: some code in this block is essentially a workaround for missing units
                # in EDD's metadata types.  Can remove this later if they're updated to use
                # consistent units following EDD-741.
                name_elt = None
                if assay_meta_type.pk == assay_time_mtype_pk:
                    custom_time_digits = self._get_time_format_string() % metadata_value
                    name_elt = '%sh' % custom_time_digits
                else:
                    name_elt = metadata_value.replace(' ', self.space_replacement)

                    # add in units if defined...otherwise, multiple numeric values are
                    # hard/impossible to distinguish from each other
                    if assay_meta_type.postfix:
                        name_elt += assay_meta_type.postfix
                name_elts.append(name_elt)

            logger.debug('Adding assay naming elements: %s' % ', '.join(name_elts))
            return self.section_separator.join(name_elts)

        except KeyError:
            raise ValueError(KeyError)  # raise more generic Exception published in the docstring


class _ExperimentDescriptionFileRow(CombinatorialDescriptionInput):
    """
    A special case of combinatorial line/assay creations in support of experiment description file
    upload. Each line of the file is itself a combinatorial line/assay creation, at
    least if protocols/times are included, or the more advanced combinatorial features used.
    One-line-per-row creation, which is what users will likely use templates for most
    often, is just a degenerate case of combinatorial creation.
    """

    def __init__(self, column_layout, cache, row_number, importer, ):
        super(_ExperimentDescriptionFileRow, self).__init__(
            _ExperimentDescNamingStrategy(column_layout, cache, importer), importer)
        self.row_number = row_number

    @property
    def base_line_name(self):
        return self.naming_strategy.base_line_name

    @base_line_name.setter
    def base_line_name(self, name):
        self.naming_strategy.base_line_name = name


class CombinatorialInputParser(object):
    def __init__(self, cache):
        self.cache = cache

    def parse(self, input_source, importer, options):
        raise NotImplementedError()  # require subclasses to implement


def _standardize_label(label):
    """
    Standardizes labeling to enable fast / flexible matching of Experiment Description column
    headers against the Protocol and MetadataType names defined in EDD's database.

    :param label: the original label.
    :return: the input text, capitalized, trimmed, and with consecutive whitespace characters
        collapsed to a single space.
    """
    return _WHITESPACE_PATTERN.sub(' ', label).strip().upper()


class ExperimentDescFileParser(CombinatorialInputParser):
    """
    File parser that takes an Experiment Description file as input and reads the contents into a
    list of CombinatorialCreationInput objects.
    """

    def __init__(self, cache):
        super(ExperimentDescFileParser, self).__init__(cache)

        # build a dict of Protocol name -> Protocol to simplify parsing
        self.protocols_by_name = {
            protocol.name.upper(): protocol
            for protocol_pk, protocol in cache.protocols.items()
        }

        # build dicts that map each metadata type name -> MetaDataType to simplify parsing.
        line_meta_types = cache.line_meta_types
        self.line_mtypes_by_name = {
            _standardize_label(meta.type_name): meta
            for pk, meta in line_meta_types.items()
        }

        self.assay_metadata_types_by_name = {
            _standardize_label(meta.type_name): meta
            for pk, meta in cache.assay_meta_types.items()
        }

        # build a list of line metadata types whose parsing isn't supported pending resolution of
        # EDD-438. Note that these *are* supported via JSON pk input, we just aren't supporting
        # lookup for now since it may be done for us, or will at least be impacted by EDD-438
        unsupported_names = [
            'Control',
            'Carbon Source(s)',
            'Line Contact',
            'Line Experimenter',
        ]
        self.unsupported_line_meta_types_by_pk = {
            pk: meta
            for pk, meta in line_meta_types.items()
            if meta.type_name in unsupported_names
        }

        # print a warning for unlikely case-sensitivity-only metadata naming differences that
        # clash with tolerant case-insensitive matching of user input in the file (which is a lot
        # more likely to be inconsistent)
        if len(self.line_mtypes_by_name) != len(line_meta_types):
            logger.warning(
                'Found some line metadata types that differ only by case. Case-insensitive '
                'matching in parsing code will arbitrarily choose one'
            )

        if len(self.assay_metadata_types_by_name) != len(cache.assay_meta_types):
            logger.warning(
                'Found some assay metadata types that differ only by case. Case-insensitive '
                'matching in this function will arbitrarily choose one'
            )

        # Note: uniqueness of protocol names is enforced by Protocol.save()... we'll trust that
        # and not print a warning here.

        # get specific metadata references needed by current parsing code
        self.ctrl_meta_type = next(
            (x for x in cache.line_meta_types.values() if x.type_name == 'Control'),
            None  # default value
        )
        self.name_meta_type = next(
            (x for x in cache.line_meta_types.values() if x.type_name == 'Line Name'),
            None  # default value
        )

        self.column_layout = None

        # true to treat all unmatched column headers as an
        # error. False to ignore unmatch line headers, but to treat those that start with a
        # protocol name and don't match an assay MetaDataType as an error
        self.REQUIRE_COL_HEADER_MATCH = True

        self.importer = None

        self.max_fractional_time_digits = 0

    def parse(self, wb, importer, options):
        logger.info('In parse(). workbook has %d sheets' % len(wb.worksheets))

        if len(wb.worksheets) == 0:
            importer.add_error(BAD_FILE_CATEGORY, subtitle=EMPTY_WORKBOOK)
            return

        # Clear out state from any previous use of this parser instance
        self.column_layout = None
        self.importer = importer

        # loop over rows
        parsed_row_inputs = []

        if len(wb.worksheets) > 1:
            sheet_name = wb.get_sheet_names()[0]
            importer.add_warning(
                IGNORED_INPUT_CATEGORY, MULTIPLE_WORKSHEETS_FOUND,
                f'All but the first sheet in your workbook, "{sheet_name}", were ignored'
            )
        worksheet = wb.worksheets[0]

        # loop over columns
        row_index = 0
        for cols_list in worksheet.iter_rows():

            logger.debug('Parsing row %d' % (row_index+1))

            # identify columns of interest first by looking for required labels
            if not self.column_layout:
                self.column_layout = self.read_column_layout(cols_list)

            # if column labels have been identified, look for line creation input data
            else:
                #  remove
                row_num = row_index + 1
                row_inputs = self.parse_row(cols_list, row_num)
                if row_inputs:  # skip empty rows
                    parsed_row_inputs.append(row_inputs)

            row_index += 1

        if not self.column_layout:
            importer.add_error(BAD_FILE_CATEGORY,
                               subtitle='No column header was found matching the single required '
                                        'value "Line Name"')
            return

        column_layout = self.column_layout

        # provide a good user-facing warning message as a reminder of line metadata types that
        # aren't supported, but were found in the input file
        unsupported_value_columns = [
            col_index
            for col_index, meta_pk in column_layout.col_index_to_line_meta_pk.items()
            if meta_pk in self.unsupported_line_meta_types_by_pk
        ]
        if unsupported_value_columns:
            line_meta_types = self.cache.line_meta_types
            unsupported_values = []
            for col_index in unsupported_value_columns:
                meta_pk = column_layout.get_line_meta_pk(col_index)
                meta_type = line_meta_types[meta_pk]
                column_letter = get_column_letter(col_index + 1)
                unsupported_values.append(f'{meta_type.type_name} (column {column_letter})')

            importer.add_warning(IGNORED_INPUT_CATEGORY, UNSUPPORTED_LINE_METADATA,
                                 ', '.join(unsupported_values))

        for combinatorial_input in parsed_row_inputs:
            combinatorial_input.fractional_time_digits = self.max_fractional_time_digits

        # after reading all rows in the file, loop over combinatorially-defined columns (as
        # defined by column headers) and remove any from our tracking that were completely empty
        # or that only contain a single value per row. This resolves a boundary condition where
        # the NamingStrategy defines these values as required for computing line names
        # combinatorially, but the lack of combinatorial values allows us to safely ignore the
        # column(s)
        for col_index in column_layout.combinatorial_col_indices:
            meta_values = set()
            multivalued_row = False

            line_mtype_pk = column_layout.col_index_to_line_meta_pk.get(col_index)

            # ignore columns that contain assay metadata, since it's not required for computing
            # line names. TODO: this creates similar boundary conditions for assay meta columns as
            # this code block attempts to resolve for lines (poor detection of empty or completely
            # single-valued columns).  Doing all the analogous introspection, caching,
            # etc for assay-related data is a more comprehensive code change we'll make later on.
            if not line_mtype_pk:
                continue

            for combinatorial_input in parsed_row_inputs:
                line_values = combinatorial_input.get_related_object_ids(line_mtype_pk)
                meta_values.update(line_values)

                if len(line_values) > 1:
                    multivalued_row = True
                    break

            if not meta_values or not multivalued_row:
                column_layout.combinatorial_col_indices.remove(col_index)

        # Enforce consistency of folder contents and filtering across all inputs.
        # This allows us to make simplifying assumptions later on when processing ICE folders
        unique_folder_ids = set()
        for index, combo in enumerate(parsed_row_inputs):

            input_folders = combo.get_related_object_ids(ICE_FOLDERS_KEY)
            if index > 0:
                for folder_id in input_folders:
                    if folder_id not in unique_folder_ids:
                        importer.add_error(INVALID_FILE_VALUE_CATEGORY, INCONSISTENT_FOLDERS,
                                           f'{folder_id} (row {combo.row_number})')
                    else:
                        current_filters = combo.folder_id_to_folders[folder_id]
                        if (set(current_filters) != set(self.folder_id_to_filters[folder_id])):
                            importer.add_error(INVALID_FILE_VALUE_CATEGORY, INCONSISTENT_FILTERS,
                                               f'row {combo.row_number}')

        return parsed_row_inputs

    def read_column_layout(self, row):
        """
        Detects the layout of a template file by matching cell contents of a row containing the
        minimal required column headers, then comparing additional headers in that row against line
        and assay metadata names in EDD's database. If required column headers aren't found in this
        row, then it is ignored. Columns can be provided in any order.
        :param row: the row to inspect for column headers
        :return: the column layout if required columns were found, or None otherwise
        """
        layout = ColumnLayout(self.importer)

        ###########################################################################################
        # loop over columns in the current row, looking for labels that identify at least the
        # minimum set of required columns
        ###########################################################################################
        found_col_labels = False
        for col_index in range(len(row)):
            cell_content = row[col_index].value

            # ignore non-string cells since they can't be the column headers we're looking for
            if not isinstance(cell_content, string_types):
                continue

            cell_content = _standardize_label(cell_content)

            # skip this cell if it has no non-whitespace content
            if not cell_content:
                continue

            #######################################################################################
            # check whether column label matches one of the fixed labels specified by the file
            # format, but where the column labels don't match a defined line MetadataType
            #######################################################################################
            if _LINE_NAME_COL_PATTERN.match(cell_content):
                layout.line_name_col = col_index
            elif _LINE_DESCRIPTION_COL_PATTERN.match(cell_content):
                layout.line_description_col = col_index
            elif _STRAIN_IDS_SINGULAR_COL_PATTERN.match(cell_content):
                layout.strain_ids_col = col_index
                layout.set_line_metadata_type(col_index, self.cache.strains_mtype, False)
            elif _STRAIN_IDS_PLURAL_COL_PATTERN.match(cell_content):
                layout.strain_ids_col = col_index
                layout.set_line_metadata_type(col_index, self.cache.strains_mtype, True)
            elif _REPLICATE_COUNT_COL_PATTERN.match(cell_content):
                layout.replicate_count_col = col_index

            # check whether the column label matches custom data defined in the database
            else:
                upper_content = cell_content.upper()

                # test whether this column is protocol-prefixed assay metadata
                assay_meta_type = self._parse_assay_metadata_header(layout,
                                                                    upper_content,
                                                                    cell_content,
                                                                    col_index)

                # if we found the type of this column, proceed to the next
                if assay_meta_type:
                    continue

                # if this column isn't protocol-prefixed, test whether it's for line metadata
                line_metadata_type = self._parse_line_metadata_header(layout,
                                                                      upper_content,
                                                                      col_index)

                # if we couldn't process this column, track a warning that describes
                # dropped columns (can be displayed later in the UI)
                if not line_metadata_type:
                    col = 'column %s' % get_column_letter(col_index+1)
                    skipped = '%(title)s" (%(column)s)' % {
                        'title': cell_content,
                        'column': col,
                    }
                    logger.warning('Bad column header "%(header)s"' % {'header': skipped})
                    is_error = self.REQUIRE_COL_HEADER_MATCH
                    logger.warning('Bad column header "%(header)s"' % {'header': skipped})
                    self.importer.add_issue(is_error, BAD_FILE_CATEGORY, INVALID_COLUMN_HEADER,
                                            skipped)

        # return the columns found in this row if at least the
        # minimum required columns were found
        found_col_labels = layout.line_name_col is not None
        if found_col_labels:
            return layout

        return None

    def _parse_assay_metadata_header(self, layout, upper_content, original_content, col_index):
        """
        :return: a truthy value if the content should be treated as assay metadata (the
            MetadataType if one was found, or True if it was clearly intended to be one, but was
            logged as an error).
        """

        ########################################################################################
        # loop over protocol names, testing for a protocol prefix in the column header
        ########################################################################################
        for upper_protocol_name, protocol in self.protocols_by_name.items():
            if upper_content.startswith(upper_protocol_name):

                # pull out the column header suffix following the protocol.
                # it should match the name of an assay metadata type
                start_index = len(upper_protocol_name)
                assay_meta_suffix = upper_content[start_index:].strip()

                suffix_meta_type = None
                is_combinatorial = False

                ################################################################################
                # loop over assay metadata types, testing for an assay metadata suffix in the
                # column header
                ################################################################################
                assay_items = self.assay_metadata_types_by_name.items()
                for upper_type_name, assay_metadata_type in assay_items:

                    # if this type has units, check whether column header matches the type name
                    # with an optional unit suffix
                    if assay_metadata_type.postfix:
                        singular_regex = _TYPE_NAME_REGEX % {
                            'type_name': upper_type_name,
                            'units': assay_metadata_type.postfix
                        }
                        suffix_meta_type = assay_metadata_type if re.match(
                                singular_regex, assay_meta_suffix, re.IGNORECASE) else None
                    # otherwise, check whether the column header exactly matches the type name
                    # (case-insensitive)
                    else:
                        # look for an exact match
                        suffix_meta_type = (assay_metadata_type
                                            if assay_meta_suffix == upper_type_name else None)

                    # if we've found the assay metadata type for this column, stop looking
                    if suffix_meta_type is not None:
                        break

                    # if the column header didn't match the assay metadata type in its
                    # raw form, look for a pluralized version of the metadata
                    # type name. Pluralization indicates the contents should be treated as a
                    # comma-delimited list of combinatorial metadata values
                    meta_regex = _PLURALIZED_REGEX % {
                        'type_name': re.escape(assay_metadata_type.type_name),
                        'units': re.escape(assay_metadata_type.postfix)
                    }
                    pluralized_match = re.match(meta_regex, assay_meta_suffix, re.IGNORECASE)

                    if pluralized_match:
                        is_combinatorial = True
                        logger.debug(
                            'column header suffix %s matched pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )
                        suffix_meta_type = assay_metadata_type
                        break
                    else:
                        logger.debug(
                            'column header suffix %s did not match pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )

                # if the column started with the name of a protocol and ended with an
                # assay metadata type name, store the association of this column with the
                # (Protocol, MetadataType) combination
                if suffix_meta_type:
                    layout.register_assay_meta_column(col_index, upper_protocol_name,
                                                      protocol, suffix_meta_type,
                                                      is_combinatorial)
                    return suffix_meta_type

                # otherwise, since the column header was prefixed with a valid
                # protocol name, assume there was a data entry error or missing metadata type
                # in the database. This check is especially important for the Time metadata
                # assumed by the file format.
                else:
                    original_case_suffix = original_content[start_index:].strip()
                    col_letter = get_column_letter(col_index+1)
                    value = '"%(suffix)s" (column %(col)s)' % {
                        'suffix': original_case_suffix,
                        'col': col_letter,
                    }
                    logger.debug("""Column header suffix %s didn't match """
                                 """ known metadata types""" % value)
                    self.importer.add_error(BAD_FILE_CATEGORY, UNMATCHED_ASSAY_COL_HEADERS_KEY,
                                            value)
                    return True

    def _parse_line_metadata_header(self, column_layout, std_cell_content, col_index):
        """
        :return: the line MetadataType if one was found or None otherwise
        """
        result = None

        # if we didn't find the singular form of the column header as line metadata, look
        # for a pluralized version that we'll treat as combinatorial line creation input
        for std_type_name, meta_type in self.line_mtypes_by_name.items():

            # check whether column header matches the type name with an optional unit suffix
            if meta_type.postfix:
                singular_regex = _TYPE_NAME_REGEX % {
                    'type_name': std_type_name, 'units': meta_type.postfix
                }
                result = (meta_type if re.match(singular_regex, std_cell_content, re.IGNORECASE)
                          else None)
            # otherwise, check whether the column header exactly matches the type name
            # (case-insensitive, whitespace collapsed in both)
            else:
                # look for an exact match
                result = meta_type if std_cell_content == std_type_name else None

            if result is not None:
                column_layout.set_line_metadata_type(col_index, result)
                return result

            # if we didn't find a singular version of the column header, check for a pluralized
            # version, which indicates that cell values should be treated as combinatorial input
            meta_regex = _PLURALIZED_REGEX % {
                'type_name': re.escape(meta_type.type_name),
                'units': re.escape(meta_type.postfix)
            }
            pluralized_match = re.match(meta_regex, std_cell_content, re.IGNORECASE)

            if pluralized_match:
                result = meta_type
                column_layout.set_line_metadata_type(col_index, result, is_combinatorial=True)
                logger.debug(
                    """Column header "%(header)s" matches line metadata type %(type)s""" % {
                        'header': std_cell_content, 'type': std_type_name
                    })
                return result

        return None

    def parse_row(self, cols_list, row_num):
        """
        Reads a single spreadsheet row to find line creation inputs. The row is read even if errors
        occur, logging errors in the 'errors' parameter so that multiple user input errors can be
        detected and communicated during a single pass of editing the file.
        """
        row_inputs = _ExperimentDescriptionFileRow(self.column_layout, self.cache, row_num,
                                                   self.importer)
        layout = self.column_layout

        ###################################################
        # Line name
        ###################################################
        cell_content = self._get_string_cell_content(
            cols_list,
            row_num,
            layout.line_name_col,
            convert_to_string=True
        )

        if cell_content:
            row_inputs.base_line_name = cell_content

        # otherwise, track error state, but keep going so we can try to detect all the
        # errors with a single pass
        else:
            # detect whether the row is completely empty...if not, raise an error. Note that a
            # completely empty row is the only case where we can safely ignore the missing
            # required value.
            for col_index, cell in enumerate(cols_list):
                if cell.value is not None and str(cell.value).strip():
                    name_col_letter = get_column_letter(layout.line_name_col + 1)
                    name_cell_num = f'{row_num}{name_col_letter}'
                    logger.info(f'Parse error: Cell {name_cell_num} was empty, but was expected '
                                'to contain a name for the EDD line. This error only occurs for '
                                'non-empty rows. ')
                    self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                            MISSING_REQUIRED_LINE_NAME,
                                            occurrence_detail=name_cell_num)
                    break

            logger.info(f'Ignored empty row {row_num}.')
            return None

        ###################################################
        # Line description
        ###################################################
        if layout.line_description_col is not None:
            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_description_col
            )
            if cell_content:
                row_inputs.description = cell_content

        ###################################################
        # Control
        ###################################################
        if layout.line_control_col is not None:

            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_control_col
            )

            if cell_content:
                tokens = cell_content.split(',')
                if len(tokens) == 1:
                    tokens = cell_content

                values = []
                for token in tokens:
                    is_control = "TRUE" == token or "YES" == cell_content
                    values.append(is_control)

                if len(values) > 1:
                    pk = self.ctrl_meta_type.pk
                    row_inputs.combinatorial_line_metadata[pk] = values

        ###################################################
        # Replicate count
        ###################################################
        if layout.replicate_count_col is not None:
            cell_content = cols_list[layout.replicate_count_col].value

            if cell_content is not None:
                try:
                    row_inputs.replicate_count = int(cell_content)
                    if row_inputs.replicate_count == 0:
                        cell = '%(row)d%(col)s' % {
                            'value': str(cell_content), 'row': row_num,
                            'col': get_column_letter(layout.replicate_count_col + 1),
                        }
                        self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, ZERO_REPLICATES, cell)
                except ValueError:
                    message = '%(value)s (%(row)d%(col)s)' % {
                        'value': str(cell_content),
                        'row': row_num,
                        'col': get_column_letter(layout.replicate_count_col+1),
                    }
                    self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, INVALID_REPLICATE_COUNT,
                                            occurrence_detail=message)
            else:
                row_inputs.replicate_count = 1
                self.importer.add_warning(IGNORED_INPUT_CATEGORY, ROWS_MISSING_REPLICATE_COUNT,
                                          row_num)

        ###################################################
        # line metadata
        ###################################################
        if layout.col_index_to_line_meta_pk:
            for col_index, line_metadata_pk in layout.col_index_to_line_meta_pk.items():
                # skip values for metadata types we've specifically disabled (with a warning)
                # earlier in the parsing process
                if line_metadata_pk in self.unsupported_line_meta_types_by_pk:
                    continue

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index,
                    convert_to_string=True
                )

                if not cell_content:
                    continue

                ###################################################
                # Strain part id(s)
                ###################################################
                # TODO: after some initial testing, consider adding custom group support for
                #  Carbon Source similar to that for strains. Since some changes to Carbon
                #  Source / Media tracking are needed, we will probably want to defer support
                # for Carbon Sources for now.
                if col_index == layout.strain_ids_col:
                    self.parse_strains_cell(layout, cols_list, row_num, row_inputs)
                    continue

                if col_index in layout.combinatorial_col_indices:
                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        line_metadata_pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        UNPARSEABLE_COMBINATORIAL_VALUE,
                    )
                else:
                    row_inputs.add_common_line_metadata(line_metadata_pk, cell_content)

        ###################################################################################
        # loop over protocol-specific columns (most likely related assay measurement times)
        ###################################################################################
        if layout.col_index_to_assay_data:

            # loop over per-protocol assay metadata columns
            columns = layout.col_index_to_assay_data.items()
            for col_index, (protocol, assay_metadata_type) in columns:

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index, convert_to_string=True
                )

                # skip blank cells
                if not cell_content:
                    continue

                # if this cell is in a column of for combinatorial input, add it to that list
                if col_index in layout.combinatorial_col_indices:
                    is_time = assay_metadata_type.pk == self.cache.assay_time_mtype.pk

                    error_key = (INCORRECT_TIME_FORMAT if is_time else
                                 UNPARSEABLE_COMBINATORIAL_VALUE)
                    parser = TIME_PARSER if is_time else RAW_STRING_PARSER

                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        assay_metadata_type.pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        error_key,
                        protocol,
                        parser
                    )

                else:
                    if row_inputs.has_assay_metadata_type(protocol.pk, assay_metadata_type.pk):
                        # TODO could improve this error content with a more complex data structure
                        self.importer.add_error(BAD_FILE_CATEGORY,
                                                DUPLICATE_ASSAY_METADATA,
                                                'col %(col_letter)s' % {
                                                    # 'protocol_id': protocol.pk,
                                                    # 'metadata_id': assay_metadata_type.pk,
                                                    'col_letter': get_column_letter(
                                                        col_index + 1),
                                                })
                    row_inputs.add_common_assay_metadata(
                        protocol.pk,
                        assay_metadata_type.pk,
                        cell_content
                    )

        return row_inputs

    def parse_strains_cell(self, layout, cols_list, row_num, row_inputs):
        strain_ids_col = layout.strain_ids_col
        col_combinatorial = strain_ids_col in layout.combinatorial_col_indices

        cell_content = self._get_string_cell_content(cols_list, row_num, strain_ids_col,
                                                     convert_to_string=True)

        combinatorial_strain_id_groups = []  # list of lists of strain IDs

        # individual part ID's that were read externally to strain groups (may be present in
        # either combinatorial or  non-combinatorial use)
        individual_strain_ids = []

        if not cell_content:
            return

        # Extract comma-delimited tokens from the cell, for fault tolerance, casting to string in
        # case user entered something else (e.g. long).
        #
        # Each valid token should be EITHER:
        # A) An ICE part ID, or
        # B) A strain group: A paren-enclosed, semicolon-delimited list of ICE part ID's. A strain
        #    group allows multiple strains to be assigned to the same line, or for lines to be
        #    created combinatorially from multiple strain groups, depending on whether the column
        #    header defines the column as allowing combinatorial entry
        tokens = str(cell_content).split(',')

        # loop over comma-delimited tokens included in the cell
        for token in tokens:
            token = token.strip()

            # ignore whitespace and multiple commas in a row
            if not token:
                continue

            # if this token is a paren-enclosed list of part numbers, it's a
            # strain group rather than single strain to be included in the list.
            token_strain_grp_match = _STRAIN_GROUP_PATTERN.match(token)

            cell_number = '%(row)s%(col)s' % {
                'row': row_num,
                'col': get_column_letter(strain_ids_col+1),
            }
            token_description = '"%(token)s" (%(cell_number)s)' % {
                'token': token, 'cell_number': cell_number,
            }

            # if token matches our regex for a strain ID group
            if token_strain_grp_match:
                logger.debug('Token "%(token)s" is a strain group. Match subgroup 1: '
                             '"%(match_grp)s"' %
                             {
                                 'token': token,
                                 'match_grp': token_strain_grp_match.group(1),
                             })
                strain_group = [strain_id.strip() for strain_id in
                                token_strain_grp_match.group(1).split(_STRAIN_GROUP_MEMBER_DELIM)]

                for strain_id in strain_group:
                    self._check_part_id_pattern(strain_id, cell_number)

                if col_combinatorial:
                    combinatorial_strain_id_groups.append(strain_group)
                else:
                    # if the column header indicates non-combinatorial use and cell contains a
                    # strain group plus any other value, then the content is badly formatted. A
                    # non-combinatorial column should only contain a single strain or list of
                    # strains. As likely as not, this is bad user input, so we'll treat it as an
                    #  error.
                    if len(tokens) > 1:
                        self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                                INCONSISTENT_COMBINATORIAL_VALUE,
                                                token_description)
                    # tolerate strain group syntax for a non-combinatorially defined part id
                    # column, since the list of included strains is equally valid whether
                    # comma-delimited or enclosed in parens and semicolon-delimited (e.g as pasted
                    # from another sheet or row)
                    else:
                        row_inputs.combinatorial_strain_id_groups.append(strain_group)

            # value isn't a valid strain group...should be an ICE part number
            else:
                logger.debug('Token "%(token)s" is NOT a strain group' % {'token': token, })
                if _STRAIN_GROUP_MEMBER_DELIM not in token:
                    individual_strain_ids.append(token)
                    self._check_part_id_pattern(token, cell_number)
                else:
                    self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                            DELIMETER_NOT_ALLOWED_VALUE, token_description)

        logger.debug('Done parsing strains for row %(row_num)s. Individual strains: %(strains)s, '
                     'combo_groups: %(combo_groups)s' % {
                        'row_num': row_num,
                        'strains': individual_strain_ids,
                        'combo_groups': combinatorial_strain_id_groups})

        # depending on whether tho column header defines combinatorial input, either submit
        # comma-delimited strains as a single group (co-culture), or for combinatorial line
        # creation
        strains_pk = self.cache.strains_mtype.pk
        if col_combinatorial:
            if individual_strain_ids:
                # tolerate any single part numbers included among strain groups by treating
                # each as its own strain id group
                for part_id in individual_strain_ids:
                    combinatorial_strain_id_groups.append([part_id])

            if combinatorial_strain_id_groups:
                row_inputs.combinatorial_line_metadata[strains_pk] = combinatorial_strain_id_groups

        else:
            if individual_strain_ids:
                row_inputs.common_line_metadata[strains_pk] = individual_strain_ids
            elif combinatorial_strain_id_groups:
                row_inputs.common_line_metadata[strains_pk] = combinatorial_strain_id_groups

    def _check_part_id_pattern(self, part_id, input_location=''):
        # test whether the strain's part number matched the expected pattern.
        # we'll allow all input through to the ICE query later in case our pattern
        # is dated, but this way we can provide a more helpful prompt for bad
        # user input
        part_number_match = TYPICAL_ICE_PART_NUMBER_PATTERN.match(part_id)

        if not part_number_match:
            opt_location_str = ' (%s)' % input_location if input_location else ''
            desc = '"%(part_id)s"%(location_str)s' % {
                               'part_id': part_id,
                               'location_str': opt_location_str,
            }
            self.importer.add_warning(POSSIBLE_USER_ERROR_CATEGORY,
                                      PART_NUMBER_PATTERN_UNMATCHED_WARNING,
                                      desc)

            logger.warning('Expected ICE part number(s)'
                           'but %s, did not match the expected pattern. This is '
                           'either bad user input, or indicates that the pattern needs '
                           'updating.' % desc)

    def _get_string_cell_content(self, row, row_num, col_index, convert_to_string=False):
        cell_content = row[col_index].value

        if cell_content is None:
            return cell_content

        if isinstance(cell_content, string_types):
            return cell_content.strip()

        else:
            actual_type = type(cell_content).__name__
            if convert_to_string:
                return str(cell_content)
            else:
                msg = '%(row)d%(col)s (value: %(value)s, type: %(type)s)' % {
                    'row': row_num,
                    'col': get_column_letter(col_index+1),
                    'type': actual_type,
                    'value': cell_content,
                }
                self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, INVALID_CELL_TYPE, msg)
                return None

    def _parse_combinatorial_input(self, row_inputs, cell_content, row_num, col_index,
                                   metadata_type_pk, error_category, error_key, protocol=None,
                                   value_parser=RAW_STRING_PARSER):
        """
        Parses the value of a single cell that may / may not have comma-separated combinatorial
        content.
        """

        for token in cell_content.split(','):
            token = token.strip()
            try:

                if value_parser == TIME_PARSER:
                    (parsed_value, fractional_digit_count) = value_parser.parse(token)
                    self.max_fractional_time_digits = max(self.max_fractional_time_digits,
                                                          fractional_digit_count)
                else:
                    parsed_value = value_parser.parse(token)

                if protocol:
                    row_inputs.add_combinatorial_assay_metadata(protocol.pk, metadata_type_pk,
                                                                parsed_value)
                else:
                    row_inputs.add_combinatorial_line_metadata(metadata_type_pk,
                                                               parsed_value)
            except ValueError:
                cell_num = '%(row_num)d%(col_letter)s' % {
                    'row_num': row_num,
                    'col_letter': get_column_letter(col_index+1)}
                logger.warning(
                    'ValueError parsing token "%(token)s" from cell content "%(value)s" in '
                    '%(cell)s' % {
                        'token': token,
                        'value': cell_content,
                        'cell': cell_num,
                    }
                )
                bad_value = '%(token)s ((cell_num)s)' % {
                    'token': token,
                    'cell_num': cell_num,
                }
                self.importer.add_error(error_category, error_key, bad_value)
                break


class JsonInputParser(CombinatorialInputParser):
    """
    Parses / verifies JSON input for combinatorial line creation. Note that instances of this
    class maintain a cache of protocols and metadata types defined in the database, so the
    lifecycle of each instance should be short.
    """
    def __init__(self, cache):
        super(JsonInputParser, self).__init__(cache)
        self.max_fractional_time_digits = 0
        self.parsed_json = None

    def parse(self, stream, importer, options):

        combinatorial_inputs = []

        if importer.errors:
            return None

        ###########################################################################################
        # Once validated, parse the JSON string into a Python dict or list
        # if validation succeeded, extract the naming strategy from the JSON, then pass the rest
        # as parameters to CombinatorialDescriptionInput. Also cache for possible later use by
        # client code (e.g. in err emails)

        parsed_json = json.loads(stream)
        self.parsed_json = parsed_json

        if not parsed_json:
            return None

        validator = Draft4Validator(JSON_SCHEMA)
        validation_errors = validator.iter_errors(parsed_json)
        for err in validation_errors:
            path_str = ('.'.join(str(elt) for elt in err.relative_path) if err.relative_path else
                        'root')
            importer.add_error(
                BAD_GENERIC_INPUT_CATEGORY, INVALID_JSON,
                f'{path_str}: {err.message}'
            )
        if importer.errors:
            logger.error('Aborting parsing due to JSON validation errors: %s' % validation_errors)
            return None

        max_decimal_digits = 0

        # tolerate either a sequence of CombinatorialDescriptionInput or a single one
        if not isinstance(parsed_json, Sequence):
            parsed_json = [parsed_json]

        for value in parsed_json:

            # work around reserved 'description' keyword in JSON schema
            description = value.pop('desc', None)

            # convert string-based keys required by JSON into their numeric equivalents to simplify
            # primary key dict lookups later on
            common_line_metadata = _copy_to_numeric_keys(value.pop(COMMON_LINE_METADATA_SECTION,
                                                                   {}))
            combinatorial_line_metadata = _copy_to_numeric_keys(
                    value.pop(COMBINATORIAL_LINE_METADATA_SECTION, {}))
            protocol_to_assay_metadata = _copy_to_numeric_keys(
                    value.pop(PROTOCOL_TO_ASSAY_METADATA_SECTION, {}))
            protocol_to_combinatorial_metadata = _copy_to_numeric_keys(
                    value.pop(PROTOCOL_TO_COMBINATORIAL_METADATA_SECTION, {}))
            ice_folder_to_filters = _copy_to_numeric_keys(value.pop('ice_folder_to_filters', {}))
            custom_name_elts = value.pop('custom_name_elts', {})

            naming_elements = value.pop(NAME_ELEMENTS_SECTION, None)
            if naming_elements:
                elements = _copy_to_numeric_elts(naming_elements[ELEMENTS_SECTION])
                abbreviations = _process_abbrev_keys(naming_elements.get(ABBREVIATIONS_SECTION,
                                                                         {}))
                naming_strategy = AutomatedNamingStrategy(importer, self.cache,
                                                          naming_elts=elements,
                                                          custom_name_elts=custom_name_elts,
                                                          abbreviations=abbreviations)
            else:
                base_name = value.pop(BASE_NAME_ELT)
                naming_strategy = _ExperimentDescNamingStrategy(ColumnLayout(self), self.cache)
                naming_strategy.base_line_name = base_name

            try:
                # just pass the JSON as initializer arguments. Won't verify the internal
                # structure/expected data types, but for starters that's probably a safe bet
                combo_input = CombinatorialDescriptionInput(
                    naming_strategy,
                    importer,
                    description=description,
                    common_line_metadata=common_line_metadata,
                    combinatorial_line_metadata=combinatorial_line_metadata,
                    protocol_to_assay_metadata=protocol_to_assay_metadata,
                    protocol_to_combinatorial_metadata=protocol_to_combinatorial_metadata,
                    ice_folder_to_filters=ice_folder_to_filters)
                combo_input.replicate_count = value.get(REPLICATE_COUNT_ELT, 1)

                # inspect JSON input to find the maximum number of decimal digits in the user input
                assay_time_pk = self.cache.assay_time_mtype.pk
                if assay_time_pk:
                    for protocol, assay_metadata in protocol_to_assay_metadata.items():
                        time_values = assay_metadata.get(assay_time_pk, [])
                        for time_value in time_values:
                            str_value = str(time_value)
                            if str_value != str((int(float(time_value)))):
                                decimal_digits = len(str_value) - str_value.find('.') - 1
                                max_decimal_digits = max(max_decimal_digits, decimal_digits)

                naming_strategy.combinatorial_input = combo_input
                combinatorial_inputs.append(combo_input)
            except RuntimeError as rte:
                logger.exception('Unexpected parse error')
                importer.add_error(INTERNAL_EDD_ERROR_CATEGORY, PARSE_ERROR, str(rte))

        # verify primary key inputs from the JSON are for the expected MetaDataType context,
        # and that they exist, since there's no runtime checking for this at database item
        # creation time when they get (necessarily) shoved into the hstore field. Note that this is
        # an non-ideal comparison of database fields cached in memory, but should be an acceptable
        # risk of having a stale cache of these values, which should be recently gathered and very
        # unlikely to be stale.

        for combo_input in combinatorial_inputs:
            combo_input.verify_pks(self.cache, importer)

        # consistently use decimal or integer time in assay names based on whether any fractional
        # input was provided
        for combo_input in combinatorial_inputs:
            combo_input.fractional_time_digits = max_decimal_digits

        return combinatorial_inputs


def _copy_to_numeric_elts(input_list):
    converted_list = []
    for index, element in enumerate(input_list):
        try:
            int_value = int(element)
            converted_list.append(int_value)
        except ValueError:
            converted_list.append(element)

    return converted_list


def _copy_to_numeric_keys(input_dict):
    converted_dict = {}
    for key, value in input_dict.items():

        # if value is a nested dict, do the same work on it
        if isinstance(value, dict):
            value = _copy_to_numeric_keys(value)
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict


def _process_abbrev_keys(input_dict):
    """
    Replaces only first-order keys with numbers for cases where they match.  This simplifies
    comparison with metadata values during line naming.  Metadata values are are always stored
    as strings, even if they're actually numeric.
    """
    converted_dict = {}
    for key, value in input_dict.items():
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict
