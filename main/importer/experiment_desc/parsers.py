# coding: utf-8
from __future__ import unicode_literals

import json
import logging
import re
from collections import Sequence

from builtins import str
from django.conf import settings
from openpyxl.utils.cell import get_column_letter
from six import string_types

from .constants import (DUPLICATE_ASSAY_METADATA, INVALID_CELL_TYPE, INVALID_REPLICATE_COUNT,
                        MISSING_REQUIRED_LINE_NAME, PARSE_ERROR,
                        PART_NUMBER_PATTERN_UNMATCHED_WARNING, ROWS_MISSING_REPLICATE_COUNT,
                        INVALID_COLUMN_HEADER, UNMATCHED_ASSAY_COL_HEADERS_KEY,
                        MULTIPLE_WORKSHEETS_FOUND, UNSUPPORTED_LINE_METADATA, EMPTY_WORKBOOK,
                        ZERO_REPLICATES, INCORRECT_TIME_FORMAT, UNPARSEABLE_COMBINATORIAL_VALUE,
                        INTERNAL_EDD_ERROR_TITLE, BAD_FILE_CATEGORY, PART_NUM_PATTERN_TITLE,
                        IGNORED_INPUT_CATEGORY, INVALID_FILE_VALUE_CATEGORY,
                        BAD_GENERIC_INPUT_CATEGORY, INCONSISTENT_COMBINATORIAL_VALUE)
from .utilities import AutomatedNamingStrategy, CombinatorialDescriptionInput, NamingStrategy

logger = logging.getLogger(__name__)

TYPICAL_ICE_PART_NUMBER_PATTERN = settings.TYPICAL_ICE_PART_NUMBER_PATTERN

###################################################################################################
# Column header patterns for the experiment description file.
# These match values generated by the ICE bulk export and also consumed by EDD's bulk line creation
# script, so don't change them arbitrarily!
LINE_NAME_COL_LABEL = 'Line\s+Name'
LINE_DESCRIPTION_COL_REGEX = 'Line\s+Description'
STRAIN_IDS_COL_LABEL = 'Part\s+ID'
REPLICATE_COUNT_COL_REGEX = 'Replicate\s+Count'

_LINE_NAME_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_NAME_COL_LABEL, re.IGNORECASE)
_LINE_DESCRIPTION_COL_PATTERN = re.compile(r'^\s*%s\s*$' % LINE_DESCRIPTION_COL_REGEX,
                                           re.IGNORECASE)
_STRAIN_IDS_SINGULAR_COL_PATTERN = re.compile(r'^\s*%s\s*$' % STRAIN_IDS_COL_LABEL, re.IGNORECASE)
_STRAIN_IDS_PLURAL_COL_PATTERN = re.compile(r'^\s*%s\s*\(s\)$' % STRAIN_IDS_COL_LABEL,
                                            re.IGNORECASE)
_REPLICATE_COUNT_COL_PATTERN = re.compile(r'^\s*%s\s*$' % REPLICATE_COUNT_COL_REGEX)
###################################################################################################

# TODO: initial pass...incorrect, but low priority
_STRAIN_GROUP_MEMBER_DELIM = ';'
_STRAIN_GROUP_REGEX = r'^\s*\(((?:\s*[^' + _STRAIN_GROUP_MEMBER_DELIM + '\)\(]+\s*' + \
                      _STRAIN_GROUP_MEMBER_DELIM + '?\s*)+)\)\s*$'
logger.info('Strain group regex: %s' % _STRAIN_GROUP_REGEX) # TODO: remove
_STRAIN_GROUP_PATTERN = re.compile(_STRAIN_GROUP_REGEX)

_TIME_VALUE_REGEX = r'^\s*(\d+(?:\.\d+)?)\s*h\s*$'
_TIME_VALUE_PATTERN = re.compile(_TIME_VALUE_REGEX, re.IGNORECASE)

# tests whether the input string ends with 's' or '(s)'
_OPT_UNIT_SUFFIX = r'(?:\s*(?:\(%(units)s\)|%(units)s))?'
_TYPE_NAME_REGEX = r'^%(type_name)s' + _OPT_UNIT_SUFFIX + '$'
_PLURALIZED_REGEX = r'^%(type_name)s(?:S|\(S\))' + _OPT_UNIT_SUFFIX + '$'


class _AssayMetadataValueParser(object):
    def parse(self, raw_value_str):
        """
        Parses the raw string input for a single assay metadata value
        :return the parsed value to store, or None if no value should be stored
        :raise ValueError if the value couldn't be parsed
        """
        raise ValueError("Choose one of the specific sub-classes to parse value")


class _RawStringValueParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        stripped = raw_value_str.strip()
        if not stripped:
            return None
        return stripped


class _DecimalTimeParser(_AssayMetadataValueParser):
    def parse(self, raw_value_str):
        match = _TIME_VALUE_PATTERN.match(raw_value_str)
        if match:
            str_value = match.group(1)
            number_value = float(str_value)  # raises ValueError as in the spec
            stripped = str(str_value.strip()).replace(',', '').replace('-', '').replace('+', '')
            sep_index = stripped.find('.')  # TODO: i18n

            fractional_digits = 0
            if sep_index >= 0:
                fractional_digits = (len(stripped) - sep_index) - 1  # TODO: commas!
            return number_value, fractional_digits
        raise ValueError(
            'Value "%s" did not match the expected time pattern (e.g. "4.0h")' % raw_value_str
        )

# stateless value parsing strategies for metadata input (time is treated specially by the file
# format)
RAW_STRING_PARSER = _RawStringValueParser()
TIME_PARSER = _DecimalTimeParser()


class ColumnLayout:
    """
    Stores column layout read from the header row of an experiment description file. Since these
    files are designed to be user edited, parsing should be as tolerant as possible.
    """

    class _CombinatorialColumnIterator(object):
        """
        A custom iterator class that helps extract the order of combinatorial line / assay 
        metadata columns from the ColumnLayout for use in automated line/assay naming. This allows
        advanced authors of Experiment Description files to control the order of naming elements in
        their lines/assays when they're performing combinatorial creation.
        """
        def __init__(self, column_layout, is_line):
            self.layout = column_layout
            self.index = 0
            self.is_line = is_line
            self.search_map = (column_layout.col_index_to_line_meta_pk if self.is_line
                               else column_layout.col_index_to_assay_data)

        def __iter__(self):
            return self

        def next(self):
            layout = self.layout
            if not (layout and layout.combinatorial_col_indices):
                raise StopIteration()

            while self.index < len(layout.combinatorial_col_indices):
                spreadsheet_col_index = layout.combinatorial_col_indices[self.index]
                self.index += 1

                if spreadsheet_col_index in self.search_map:
                    return spreadsheet_col_index

            if self.index >= len(layout.combinatorial_col_indices):
                raise StopIteration()

    def __init__(self, importer):
        self.line_name_col = None
        self.line_description_col = None
        self.line_control_col = None
        self.replicate_count_col = None
        self.strain_ids_col = None
        self.col_index_to_line_meta_pk = {}
        self.col_index_to_assay_data = {}  # maps col index -> (Protocol, MetadataType)

        # indices of all *any* columns for combinatorial creation (both metadata AND strains!)
        self.combinatorial_col_indices = []
        self.unique_assay_protocols = {}
        self.importer = importer

    def register_protocol(self, protocol):
        self.unique_assay_protocols[protocol.pk] = True

    def has_assay_metadata(self, upper_protocol_name, metadata_pk):
        """
        Tests whether any columns have been detected so far that store a specific (Protocol,
        MetadataType) pair.

        :param upper_protocol_name:
        :param metadata_pk:
        :return:
        """
        items = self.col_index_to_assay_data.items()
        for col_index, (existing_protocol, existing_assay_meta_type) in items:
            if ((upper_protocol_name == existing_protocol.name.upper()) and
                    (metadata_pk == existing_assay_meta_type.pk)):
                return True
        return False

    def combinatorial_line_col_order(self):
        return ColumnLayout._CombinatorialColumnIterator(self, True)

    def combinatorial_assay_col_order(self):
        return ColumnLayout._CombinatorialColumnIterator(self, False)

    def register_assay_meta_column(self, col_index, upper_protocol_name, protocol, assay_meta_type,
                                   is_combinatorial):

        # test for duplicate use of the same (Protocol, MetadataType) combination.
        # if we see it, log an error -- no clear/automated way for us to resolve which column
        #  has the correct values!
        if self.has_assay_metadata(upper_protocol_name, assay_meta_type.pk):
            self.importer.add_error(BAD_FILE_CATEGORY, DUPLICATE_ASSAY_METADATA, assay_meta_type.pk)

        self.register_protocol(protocol)

        self.col_index_to_assay_data[col_index] = (protocol, assay_meta_type)
        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)

        logger.debug(
            'Column %(col)d matches protocol "%(protocol)s", assay metadata type "%(meta_type)s"' %
            {
                'col': col_index + 1,
                'protocol': protocol.name,
                'meta_type': assay_meta_type.type_name
            }
        )

    def get_assay_metadata_type(self, col_index):
        value = self.col_index_to_assay_data.get(col_index, None)
        if not value:
            return None

        return value[1]

    def get_line_metadata_type(self, col_index):
        return self.col_index_to_line_meta_pk.get(col_index, None)

    @property
    def unique_protocols(self):
        return self.unique_assay_protocols.keys()

    def set_line_metadata_type(self, col_index, line_metadata_type, is_combinatorial=False):
        logger.debug(
            'Column %d matches line metadata type %s' %
            (col_index+1, line_metadata_type.type_name)
        )

        self.col_index_to_line_meta_pk[col_index] = line_metadata_type.pk

        if is_combinatorial:
            self.combinatorial_col_indices.append(col_index)


class _ExperimentDescNamingStrategy(NamingStrategy):
    """
    A simple line/assay naming strategy assumed in the experiment description file use case,
    where line names/assay names are created automatically by from a combination of the base 
    line name, plus metadata values for any combinatorially-defined columns that are needed to
    make resulting line/assay names unique. Combinatorial metadata values included in the names are
    added in the order that columns were specified in the file.
    Note that this allows for duplicate assay names within different
    protocols, which should be clear in EDD's UI from protocol filtering and unit markings in the
    visualizations.
    """

    def __init__(self, col_layout, assay_time_metadata_type_pk):
        super(_ExperimentDescNamingStrategy, self).__init__()
        self.col_layout = col_layout
        self.base_line_name = None
        self.assay_time_metadata_type_pk = assay_time_metadata_type_pk

    def get_line_name(self, line_strain_ids, line_metadata, replicate_num, line_metadata_types,
                      combinatorial_metadata_types, is_control, strains_by_pk):
        """
        Computes the line name, either by using the explicitly-proveded name from the file, OR if 
        there are combinatorially-defined columns (by appending an 's' or '(s)' to the column 
        header), by iterating over combinatorial columns in the order defined by the file, 
        then appending combinatorial metadata values to the line name.  Note that if used, replicate
        number is always at the end regardless of column order.
        """

        # iterate over combinatorial line metadata columns and construct line name in the same order
        # that name-relevant elements were listed in columns in the input file. We have to include
        # values for the combinatorial metadata so that line names will be unique
        layout = self.col_layout
        name_elts = []
        included_base_name = False
        combinatorial_strains = self.names_contain_strains()

        # build the name segment for strains needed to make this line name unique (if any)
        strain_names_list = self._build_strains_names_list(line_strain_ids, strains_by_pk)
        strains_str = self.multivalue_separator.join(strain_names_list) if strain_names_list else ''
        logger.debug('Strains : %s' % str())

        included_strain_names = False
        for line_metadata_col in layout.combinatorial_line_col_order():

            # if the base line name hasn't been included yet, and comes before this metadata
            # element, insert it
            if (not included_base_name) and (layout.line_name_col < line_metadata_col):
                included_base_name = True
                name_elts.append(self.base_line_name)

            if (combinatorial_strains and not included_strain_names) and (
                        layout.strain_ids_col < line_metadata_col):
                included_strain_names = True
                if strains_str:
                    name_elts.append(strains_str)

            line_meta_pk = self.col_layout.get_line_metadata_type(line_metadata_col)

            metadata_value = line_metadata.get(line_meta_pk, None)  # value is optional!

            if not metadata_value:
                # TODO: add a warning that line names won't be consistent
                continue

            line_meta_type = line_metadata_types[line_meta_pk]
            naming_elt = metadata_value
            if line_meta_type.postfix:
                naming_elt += line_meta_type.postfix
            name_elts.append(naming_elt.replace(' ', self.space_replacement))

        # if the base line name still isn't added, add it
        if not included_base_name:
            name_elts.append(self.base_line_name)
            included_base_name = True

        # if strain id(s) needed for uniqueness, and aren't added, add them
        if combinatorial_strains and not included_strain_names and strains_str:
            name_elts.append(strains_str)
            included_strain_names = True

        # if creating more than one replicate, build a suffix to show replicate number so that
        # line names are unique. Replicate number should always be last in the line name
        if self.combinatorial_input.replicate_count > 1:
            replicate_suffix = 'R%d' % replicate_num
            name_elts.append(replicate_suffix)

        logger.debug('Building line name from elements: %s' % str(name_elts))

        # if making lines combinatorially based on line metadata, insert the combinatorial values
        # into the line name so that line names will be unique
        name = self.section_separator.join(name_elts)

        logger.debug('Name is %s' % name)
        return name

    def names_contain_strains(self):
        return self.col_layout.strain_ids_col in self.col_layout.combinatorial_col_indices

    def _get_time_format_string(self):
        if self.fractional_time_digits:
            return '%0.' + ('%d' % self.fractional_time_digits + 'f')
        return '%d'

    def get_assay_name(self, line, protocol_pk, assay_metadata, assay_metadata_types):
        layout = self.col_layout
        name_elts = [line.name]

        logger.debug(
            'Combinatorial assay column order: %s' % ','.join([str(index) for index in
                    layout.combinatorial_assay_col_order()]))

        try:

            # iterate over combinatorial assay metadata columns and construct assay name in the same
            # order that name-relevant elements were listed in columns in the input file. We have to
            # include values for the combinatorial metadata so that assay names will be unique
            for assay_metadata_col in layout.combinatorial_assay_col_order():
                col_protocol, assay_meta_type = layout.col_index_to_assay_data[assay_metadata_col]

                # if this column contained metadata for a different protocol, skip it
                if col_protocol.pk != protocol_pk:
                    continue

                logger.debug(
                    'Inspecting combinatorial assay metadata column %(col)s for protocol '
                    '"%(protocol)s", meta "%(meta_type)s"' % {
                        'col': get_column_letter(assay_metadata_col + 1),
                        'protocol': col_protocol.name,
                        'meta_type': assay_meta_type.type_name})

                metadata_value = assay_metadata.get(assay_meta_type.pk)

                # TODO: some code in this block is essentially a workaround for missing units
                # in EDD's metadata types.  Can remove this later if they're updated to use
                # consistent units following EDD-741.
                name_elt = None
                if assay_meta_type.pk == self.assay_time_metadata_type_pk:
                    custom_time_digits = self._get_time_format_string() % metadata_value
                    name_elt = '%sh' % custom_time_digits
                else:
                    name_elt = metadata_value.replace(' ', self.space_replacement)

                    # add in units if defined...otherwise, multiple numeric values are
                    # hard/impossible to distinguish from each other
                    if assay_meta_type.postfix:
                        name_elt += assay_meta_type.postfix
                name_elts.append(name_elt)

            logger.debug('Adding assay naming elements: %s' % ', '.join(name_elts))
            return self.section_separator.join(name_elts)

        except KeyError:
            raise ValueError(KeyError)  # raise more generic Exception published in the docstring


class _InputFileRow(CombinatorialDescriptionInput):
    """
    A special case of combinatorial line/assay creations in support of experiment description file
    upload. Each line of the file is itself a combinatorial line/assay creation, at
    least if protocols/times are included, or the more advanced combinatorial features used.
    One-line-per-row creation, which is what users will likely use templates for most
    often, is just a degenerate case of combinatorial creation.
    """

    def __init__(self, column_layout, assay_time_meta_pk, row_number):
        super(_InputFileRow, self).__init__(
                _ExperimentDescNamingStrategy(column_layout, assay_time_meta_pk))
        self.row_number = row_number

    @property
    def base_line_name(self):
        return self.naming_strategy.base_line_name

    @base_line_name.setter
    def base_line_name(self, name):
        self.naming_strategy.base_line_name = name


class CombinatorialInputParser(object):
    def __init__(self, protocols, line_metadata_types_by_pk, assay_metadata_types_by_pk):

        # if the metadata type is present in the database, construct a parser for assay time
        # (we need a pk to store it, and the parser
        assay_time_type = None
        for pk, metadata_type in assay_metadata_types_by_pk.items():
            if metadata_type.type_name.upper() == 'TIME':
                assay_time_type = metadata_type
                break

        self.assay_time_meta_pk = assay_time_type.pk

    def parse(self, input_source, importer):
        raise NotImplementedError()  # require subclasses to implement


class ExperimentDescFileParser(CombinatorialInputParser):
    """
    File parser that takes a study "template file" as input and reads the contents into a list of
    CombinatorialCreationInput objects.
    """

    def __init__(self, protocols_by_pk, line_metadata_types_by_pk, assay_metadata_types_by_pk):
        super(ExperimentDescFileParser, self).__init__(protocols_by_pk, line_metadata_types_by_pk,
                                                       assay_metadata_types_by_pk)

        self.line_metadata_types_by_pk = line_metadata_types_by_pk

        # build a dict of Protocol name -> Protocol to simplify parsing
        self.protocols_by_name = {
            protocol.name.upper(): protocol
            for protocol_pk, protocol in protocols_by_pk.iteritems()
        }

        # build dicts that map each metadata type name -> MetaDataType to simplify parsing.
        # TODO: revisit these with latest parsing code...may be unnecessary following regex-based
        # parsing improvements
        self.line_metadata_types_by_name = {
            meta.type_name.upper(): meta
            for pk, meta in line_metadata_types_by_pk.iteritems()
        }

        self.assay_metadata_types_by_name = {
            meta.type_name.upper(): meta
            for pk, meta in assay_metadata_types_by_pk.iteritems()
        }

        # build a list of line metadata types whose parsing isn't supported pending resolution of
        # EDD-438. Note that these *are* supported via JSON pk input, we just aren't supporting
        # lookup for now since it may be done for us, or will at least be impacted by EDD-438
        self.unsupported_line_meta_types_by_pk = {
            pk: meta for pk, meta in line_metadata_types_by_pk.iteritems() if meta.type_name in
            ['Control', 'Carbon Source(s)', 'Line Contact', 'Line Experimenter', ]
        }

        # print a warning for unlikely case-sensitivity-only metadata naming differences that
        # clash with tolerant case-insensitive matching of user input in the file (which is a lot
        # more likely to be inconsistent)
        if len(self.line_metadata_types_by_name) != len(line_metadata_types_by_pk):
            logger.warning(
                'Found some line metadata types that differ only by case. Case-insensitive '
                'matching in parsing code will arbitrarily choose one'
            )

        if len(self.assay_metadata_types_by_name) != len(assay_metadata_types_by_pk):
            logger.warning(
                'Found some assay metadata types that differ only by case. Case-insensitive '
                'matching in this function will arbitrarily choose one'
            )

        # Note: uniqueness of protocol names is enforced by Protocol.save()... we'll trust that
        # and not print a warning here.

        self.column_layout = None

        # true to treat all unmatched column headers as an
        # error. False to ignore unmatch line headers, but to treat those that start with a
        # protocol name and don't match an assay MetaDataType as an error
        self.REQUIRE_COL_HEADER_MATCH = True

        # if the metadata type is present in the database, construct a parser for assay time
        # (we need a pk to store it, and the parser
        assay_time_type = self.assay_metadata_types_by_name.get('TIME', None)
        self.assay_time_meta_pk = assay_time_type.pk if assay_time_type else None

        self.importer = None

        self.max_fractional_time_digits = 0

    def parse(self, wb, importer):
        logger.warning('In parse(). workbook has %d sheets' % len(wb.worksheets))

        if len(wb.worksheets) == 0:
            importer.add_error(BAD_FILE_CATEGORY, subtitle=EMPTY_WORKBOOK)
            return

        # Clear out state from any previous use of this parser instance
        self.column_layout = None
        self.importer = importer

        # loop over rows
        parsed_row_inputs = []

        if len(wb.worksheets) > 1:
            sheet_name = wb.get_sheet_names[0]
            importer.add_warning(IGNORED_INPUT_CATEGORY, MULTIPLE_WORKSHEETS_FOUND,
                                 'All but the first sheet, "%(sheet_name)s", were ignored' % {
                                     'sheet_name': sheet_name,
                                 })
        worksheet = wb.worksheets[0]

        # loop over columns
        row_index = 0
        for cols_list in worksheet.iter_rows():

            logger.debug('Examining row %d' % (row_index+1))

            # identify columns of interest first by looking for required labels
            if not self.column_layout:
                self.column_layout = self.read_column_layout(cols_list)

            # if column labels have been identified, look for line creation input data
            else:
                #  remove
                row_num = row_index + 1
                row_inputs = self.read_row(cols_list, row_num)
                if row_inputs:
                    parsed_row_inputs.append(row_inputs)

            row_index += 1

        if not self.column_layout:
            importer.add_error(BAD_FILE_CATEGORY,
                               subtitle='No column header was found matching the single required '
                                        'value "Line Name"')
            return

        column_layout = self.column_layout

        # provide a good user-facing warning message as a reminder of line metadata types that
        # aren't supported, but were found in the input file
        unsupported_value_columns = [col_index for col_index, meta_pk in
                                     column_layout.col_index_to_line_meta_pk.iteritems()
                                     if meta_pk in self.unsupported_line_meta_types_by_pk]
        if unsupported_value_columns:
            unsupported_values = []
            for col_index in unsupported_value_columns:
                meta_pk = column_layout.get_line_metadata_type(col_index)
                meta_type = self.line_metadata_types_by_pk[meta_pk]
                value = '"%(name)s" (column %(col)s)' % {
                            'name': meta_type.type_name,
                            'col': get_column_letter(col_index+1)}
                unsupported_values.append(value)

            importer.add_warning(IGNORED_INPUT_CATEGORY, UNSUPPORTED_LINE_METADATA,
                                 ', '.join(unsupported_values))

        for combinatorial_input in parsed_row_inputs:
            combinatorial_input.fractional_time_digits = self.max_fractional_time_digits

        return parsed_row_inputs

    def read_column_layout(self, row):
        """
        Detects the layout of a template file by matching cell contents of a row containing the
        minimal required column headers, then comparing additional headers in that row against line
        and assay metadata names in EDD's database. If required column headers aren't found in this
        row, then it is ignored. Columns can be provided in any order.
        :param row: the row to inspect for column headers
        :return: the column layout if required columns were found, or None otherwise
        """
        logger.debug('in read_column_layout()')
        layout = ColumnLayout(self.importer)

        ###########################################################################################
        # loop over columns in the current row, looking for labels that identify at least the
        # minimum set of required columns
        ###########################################################################################
        found_col_labels = False
        for col_index in range(len(row)):
            cell_content = row[col_index].value

            # ignore non-string cells since they can't be the column headers we're looking for
            if not isinstance(cell_content, string_types):
                continue

            cell_content = cell_content.strip()

            # skip this cell if it has no non-whitespace content
            if not cell_content:
                continue

            #######################################################################################
            # check whether column label matches one of the fixed labels specified by the file
            # format
            #######################################################################################
            if _LINE_NAME_COL_PATTERN.match(cell_content):
                layout.line_name_col = col_index
            elif _LINE_DESCRIPTION_COL_PATTERN.match(cell_content):
                layout.line_description_col = col_index
            elif _STRAIN_IDS_SINGULAR_COL_PATTERN.match(cell_content):
                layout.strain_ids_col = col_index
            elif _STRAIN_IDS_PLURAL_COL_PATTERN.match(cell_content):
                layout.strain_ids_col = col_index
                layout.combinatorial_col_indices.append(col_index)
            elif _REPLICATE_COUNT_COL_PATTERN.match(cell_content):
                layout.replicate_count_col = col_index

            # check whether the column label matches custom data defined in the database
            else:
                upper_content = cell_content.upper().strip()

                # test whether this column is protocol-prefixed assay metadata
                assay_meta_type = self._parse_assay_metadata_header(
                    layout,
                    upper_content,
                    col_index
                )
                # if we found the type of this column, proceed to the next
                if assay_meta_type:
                    continue
                # if this column isn't protocol-prefixed, test whether it's for line metadata
                line_metadata_type = self._parse_line_metadata_header(
                    layout,
                    upper_content,
                    col_index
                )

                # if we couldn't process this column, track a warning that describes
                # dropped columns (can be displayed later in the UI)
                if line_metadata_type is None:
                    col = 'col %s' % get_column_letter(col_index+1)
                    skipped = '"%(title)s" (%(column)s)' % {
                        'title': cell_content,
                        'column': col,
                    }
                    logger.warning('Bad column header "%(header)s"' % {'header': skipped})
                    is_error = self.REQUIRE_COL_HEADER_MATCH
                    logger.warning('Bad column header "%(header)s"' % {'header': skipped})
                    self.importer.add_issue(is_error, BAD_FILE_CATEGORY, INVALID_COLUMN_HEADER,
                                            skipped)

        # test whether we've located all the required columns
        found_col_labels = layout.line_name_col is not None

        # return the columns found in this row if at least the
        # minimum required columns were found
        if found_col_labels:
            logger.debug('Done with read_column_layout()')
            return layout

        return None

    def _parse_assay_metadata_header(self, layout, upper_content, col_index):
        """
        :return: a truthy value if the content should be treated as assay metadata (the
            MetadataType if one was found, or True if it was clearly intended to be one, but was
            logged as an error).
        """

        ########################################################################################
        # loop over protocol names, testing for a protocol prefix in the column header
        ########################################################################################
        for upper_protocol_name, protocol in self.protocols_by_name.items():
            if upper_content.startswith(upper_protocol_name):

                # pull out the column header suffix following the protocol.
                # it should match the name of an assay metadata type
                assay_meta_suffix = upper_content[len(upper_protocol_name):].strip()

                suffix_meta_type = None
                is_combinatorial = False

                ################################################################################
                # loop over assay metadata types, testing for an assay metadata suffix in the
                # column header
                ################################################################################
                for upper_type_name, assay_metadata_type in \
                    self.assay_metadata_types_by_name.items():

                    # if this type has units, check whether column header matches the type name
                    # with an optional unit suffix
                    if assay_metadata_type.postfix:
                        singular_regex = _TYPE_NAME_REGEX % {
                            'type_name': upper_type_name,
                            'units': assay_metadata_type.postfix
                        }
                        suffix_meta_type = assay_metadata_type if re.match(
                                singular_regex, assay_meta_suffix, re.IGNORECASE) else None
                    # otherwise, check whether the column header exactly matches the type name
                    # (case-insensitive)
                    else:
                        # look for an exact match
                        suffix_meta_type = (assay_metadata_type
                                            if assay_meta_suffix == upper_type_name else None)

                    # if we've found the assay metadata type for this column, stop looking
                    if suffix_meta_type is not None:
                        break

                    # if the column header didn't match the assay metadata type in its
                    # raw form, look for a pluralized version of the metadata
                    # type name. Pluralization indicates the contents should be treated as a
                    # comma-delimited list of combinatorial metadata values
                    meta_regex = _PLURALIZED_REGEX % {
                        'type_name': re.escape(assay_metadata_type.type_name),
                        'units': re.escape(assay_metadata_type.postfix)
                    }
                    pluralized_match = re.match(meta_regex, assay_meta_suffix, re.IGNORECASE)

                    if pluralized_match:
                        is_combinatorial = True
                        logger.debug(
                            'column header suffix %s matched pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )
                        suffix_meta_type = assay_metadata_type
                        break
                    else:
                        logger.debug(
                            'column header suffix %s did not match pluralized regex %s' %
                            (assay_meta_suffix, meta_regex)
                        )

                # if the column started with the name of a protocol and ended with an
                # assay metadata type name, store the association of this column with the
                # (Protocol, MetadataType) combination
                if suffix_meta_type:
                    layout.register_assay_meta_column(col_index, upper_protocol_name,
                                                      protocol, suffix_meta_type,
                                                      is_combinatorial)
                    return suffix_meta_type

                # otherwise, since the column header was prefixed with a valid
                # protocol name, assume there was a data entry error or missing metadata type
                # in the database. This check is especially important for the Time metadata
                # assumed by the file format.
                else:
                    col_letter = get_column_letter(col_index+1)
                    logger.debug("""Column header suffix "%s" didn't match known metadata types""")
                    self.importer.add_error(BAD_FILE_CATEGORY,
                                            UNMATCHED_ASSAY_COL_HEADERS_KEY, col_letter)
                    return True

    def _parse_line_metadata_header(self, column_layout, upper_content, col_index):
        """
        :return: the line MetadataType if one was found or None otherwise
        """
        result = None

        # if we didn't find the singular form of the column header as line metadata, look
        # for a pluralized version that we'll treat as combinatorial line creation input
        for upper_type_name, meta_type in self.line_metadata_types_by_name.items():

            # check whether column header matches the type name with an optional unit suffix
            if meta_type.postfix:
                singular_regex = _TYPE_NAME_REGEX % {
                    'type_name': upper_type_name, 'units': meta_type.postfix
                }
                result = (meta_type if re.match(singular_regex, upper_content, re.IGNORECASE)
                          else None)
            # otherwise, check whether the column header exactly matches the type name
            # (case-insensitive)
            else:
                # look for an exact match
                result = (
                    meta_type if upper_content == upper_type_name else None)

            if result is not None:
                column_layout.set_line_metadata_type(col_index, result)
                return result

            # if we didn't find a singular version of the column header, check for a pluralized
            # version, which indicates that cell values should be treated as combinatorial input
            meta_regex = _PLURALIZED_REGEX % {
                'type_name': re.escape(meta_type.type_name),
                'units': re.escape(meta_type.postfix)
            }
            pluralized_match = re.match(meta_regex, upper_content, re.IGNORECASE)

            if pluralized_match:
                result = meta_type
                column_layout.set_line_metadata_type(col_index, result, is_combinatorial=True)
                return result

                logger.debug("""Column header "%(header)s" matches line metadata type %(type)s"""
                             % { 'header': upper_content,
                                 'type': upper_type_name})

        return None

    def read_row(self, cols_list, row_num):
        """
        Reads a single spreadsheet row to find line creation inputs. The row is read even if errors
        occur, logging errors in the 'errors' parameter so that multiple user input errors can be
        detected and communicated during a single pass of editing the file.
        :param layout: the column header layout read from the beginning of the file. Informs this
            method which optional columns have been defined, as well as what order the columns are
            in (arbitrary column order is supported).
        """
        row_inputs = _InputFileRow(self.column_layout, self.assay_time_meta_pk, row_num)
        layout = self.column_layout

        ###################################################
        # Line name
        ###################################################
        cell_content = self._get_string_cell_content(
            cols_list,
            row_num,
            layout.line_name_col,
            convert_to_string=True
        )

        if cell_content:
            row_inputs.base_line_name = cell_content

        # otherwise, track error state, but keep going so we can try to detect all the
        # errors with a single pass
        else:
            logger.debug(
                'Parse error: Cell %(row_num)d%(col)s was empty, but was expected '
                'to contain a name for the EDD line.' % {
                    'row_num': row_num,
                    'col': get_column_letter(layout.line_name_col + 1),
                }
            )
            self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, MISSING_REQUIRED_LINE_NAME,
                                    occurrence_detail=row_num)

        ###################################################
        # Line description
        ###################################################
        if layout.line_description_col is not None:
            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_description_col
            )
            if cell_content:
                row_inputs.description = cell_content

        ###################################################
        # Control
        ###################################################
        if layout.line_control_col is not None:

            cell_content = self._get_string_cell_content(
                cols_list,
                row_num,
                layout.line_control_col
            )

            if cell_content:
                tokens = cell_content.split(',')
                if len(tokens) == 1:
                    tokens = cell_content

                values = []
                for token in tokens:
                    is_control = "TRUE" == token or "YES" == cell_content
                    values.append(is_control)
                row_inputs.is_control = values

        ###################################################
        # Replicate count
        ###################################################
        if layout.replicate_count_col is not None:
            cell_content = cols_list[layout.replicate_count_col].value

            if cell_content is not None:
                try:
                    row_inputs.replicate_count = int(cell_content)
                    if row_inputs.replicate_count == 0:
                        cell = '%(row)d%(col)s' % {
                            'value': str(cell_content), 'row': row_num,
                            'col': get_column_letter(layout.replicate_count_col + 1),
                        }
                        self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, ZERO_REPLICATES, cell)
                except ValueError:
                    message = '%(value)s (%(row)d%(col)s)' % {
                        'value': str(cell_content),
                        'row': row_num,
                        'col': get_column_letter(layout.replicate_count_col+1),
                    }
                    self.importer.add_error(BAD_GENERIC_INPUT_CATEGORY, INVALID_REPLICATE_COUNT,
                                            occurrence_detail=message)
            else:
                row_inputs.replicate_count = 1
                self.importer.add_warning(IGNORED_INPUT_CATEGORY, ROWS_MISSING_REPLICATE_COUNT,
                                          row_num)

        ###################################################
        # Strain part id(s)
        ###################################################
        # TODO: after some initial testing, consider adding custom group support for Carbon Source
        # similar to that for strains. Since some changes to Carbon Source / Media tracking are
        # needed, we will probably want to defer support for Carbon Sources for now.
        if layout.strain_ids_col is not None:
            self.parse_strains(layout, cols_list, row_num, row_inputs)

        ###################################################
        # line metadata
        ###################################################
        if layout.col_index_to_line_meta_pk:
            for col_index, line_metadata_pk in layout.col_index_to_line_meta_pk.items():
                # skip values for metadata types we've specifically disabled (with a warning)
                # earlier in the parsing process
                if line_metadata_pk in self.unsupported_line_meta_types_by_pk:
                    continue

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index,
                    convert_to_string=True
                )

                if col_index in layout.combinatorial_col_indices:
                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        line_metadata_pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        UNPARSEABLE_COMBINATORIAL_VALUE,
                    )
                else:
                    row_inputs.add_common_line_metadata(line_metadata_pk, cell_content)

        ###################################################################################
        # loop over protocol-specific columns (most likely related assay measurement times)
        ###################################################################################
        if layout.col_index_to_assay_data:

            # loop over per-protocol assay metadata columns
            for col_index, (protocol, assay_metadata_type) in \
                    layout.col_index_to_assay_data.items():

                cell_content = self._get_string_cell_content(
                    cols_list,
                    row_num,
                    col_index, convert_to_string=True
                )

                # skip blank cells
                if not cell_content:
                    continue

                # if this cell is in a column of for combinatorial input, add it to that list
                if col_index in layout.combinatorial_col_indices:
                    is_time = assay_metadata_type.pk == self.assay_time_meta_pk

                    error_key = (INCORRECT_TIME_FORMAT if is_time else
                                 UNPARSEABLE_COMBINATORIAL_VALUE)
                    parser = TIME_PARSER if is_time else RAW_STRING_PARSER

                    self._parse_combinatorial_input(
                        row_inputs,
                        cell_content,
                        row_num,
                        col_index,
                        assay_metadata_type.pk,
                        INVALID_FILE_VALUE_CATEGORY,
                        error_key,
                        protocol,
                        parser
                    )

                else:
                    if row_inputs.has_assay_metadata_type(protocol.pk, assay_metadata_type.pk):
                        # TODO could improve this error content with a more complex data structure
                        self.importer.add_error(BAD_FILE_CATEGORY,
                                                DUPLICATE_ASSAY_METADATA,
                                                'col %(col_letter)s' % {
                                                    # 'protocol_id': protocol.pk,
                                                    # 'metadata_id': assay_metadata_type.pk,
                                                    'col_letter': get_column_letter(
                                                        col_index + 1),
                                                })
                    row_inputs.add_common_assay_metadata(
                        protocol.pk,
                        assay_metadata_type.pk,
                        cell_content
                    )

        return row_inputs

    def parse_strains(self, layout, cols_list, row_num, row_inputs):
        strain_ids_col = layout.strain_ids_col
        is_combinatorial = strain_ids_col in layout.combinatorial_col_indices

        cell_content = self._get_string_cell_content(cols_list, row_num,
                                                strain_ids_col, convert_to_string=True)

        # build a list of strain ids for this input
        individual_strain_ids = []

        if not cell_content:
            return

        # cast to string in case user entered something else (e.g. long)
        tokens = str(cell_content).split(',')

        # loop over comma-delimited tokens included in the cell
        for token in tokens:
            token = token.strip()

            if not token:
                continue

            # if this token is a paren-enclosed list of part numbers, it's a
            # strain group rather than single strain to be
            # included in the list. That means that each top-level comma-delimited
            # entry in the list will result in creation of at least one line
            strain_group_match = _STRAIN_GROUP_PATTERN.match(token)

            if strain_group_match:
                strain_group = [strain_id.strip() for strain_id in
                                strain_group_match.group(1).split(_STRAIN_GROUP_MEMBER_DELIM)]
                if is_combinatorial:
                    logger.info('Found strain id group %(group)s in cell %(row)d%(col)s' % {
                        'group': strain_group,
                        'row': row_num,
                        'col': get_column_letter(strain_ids_col),
                    })
                    row_inputs.combinatorial_strain_id_groups.append(strain_group)
                else:
                    # if we've already seen a strain or strain group in this cell, the content
                    # is badly formatted, since a non-combinatorial column should only contain
                    # a single strain or list of strains. As likely as not, this is bad user
                    # input, so we'll treat it as an error.
                    if row_inputs.combinatorial_strain_id_groups:
                        bad_value = '"%(token)s" (%(row)s%(col)s)' % {
                            'token': cell_content,
                            'row': row_num,
                            'col': get_column_letter(strain_ids_col),
                        }
                        self.importer.add_error(INVALID_FILE_VALUE_CATEGORY,
                                                INCONSISTENT_COMBINATORIAL_VALUE, bad_value)
                    else:
                        row_inputs.combinatorial_strain_id_groups.append(strain_group)
                        for strain_id in strain_group:
                            self._check_part_id_pattern(strain_id)
            else:
                individual_strain_ids.append(token)
                self._check_part_id_pattern(token)

        # depending on whether tho column header defines combinatorial input, either submit
        # comma-delimited strains as a single group (co-culture), or for combinatorial line
        # creation
        if is_combinatorial:
            for strain_id in individual_strain_ids:
                row_inputs.combinatorial_strain_id_groups.append([strain_id,])

        elif individual_strain_ids:
            row_inputs.combinatorial_strain_id_groups.append(individual_strain_ids)

    def _check_part_id_pattern(self, part_id, row_num=None):
        # test whether the strain's part number matched the expected pattern.
        # we'll allow all input through to the ICE query later in case our pattern
        # is dated, but this way we can provide a more helpful prompt for bad
        # user input
        part_number_match = TYPICAL_ICE_PART_NUMBER_PATTERN.match(part_id)

        if not part_number_match:
            quoted_content = '"%s"' % part_id
            self.importer.add_warning(PART_NUM_PATTERN_TITLE, PART_NUMBER_PATTERN_UNMATCHED_WARNING,
                                      quoted_content)
            file_row = 'in row %d' % row_num if row_num else ''
            logger.warning('Expected ICE part number(s)%(row_num)s, '
                           'but "%(part_id)s" did not match the expected pattern. This is '
                           'either bad user input, or indicates that the pattern needs '
                           'updating.' % {
                               'row_num': file_row, 'part_id': part_id
                           })

    def _get_string_cell_content(self, row, row_num, col_index, convert_to_string=False):
        cell_content = row[col_index].value

        if cell_content is None:
            return cell_content

        if isinstance(cell_content, string_types):
            return cell_content.strip()

        else:
            actual_type = type(cell_content).__name__
            if convert_to_string:
                return str(cell_content)
            else:
                msg = '%(row)d%(col)s (value: %(value)s, type: %(type)s)' % {
                    'row': row_num,
                    'col': get_column_letter(col_index+1),
                    'type': actual_type,
                    'value': cell_content,
                }
                self.importer.add_error(INVALID_FILE_VALUE_CATEGORY, INVALID_CELL_TYPE, msg)
                return None

    def _parse_combinatorial_input(self, row_inputs, cell_content, row_num, col_index,
                                   metadata_type_pk, error_category, error_key, protocol=None,
                                   value_parser=RAW_STRING_PARSER):
        """
        Parses the value of a single cell that may / may not have comma-separated combinatorial
        content.
        :param row_inputs:
        :param cell_content:
        :param row_num:
        :param col_index:
        :param metadata_type_pk:
        :param error_key:
        :param protocol:
        :param value_parser:
        :return:
        """

        for token in cell_content.split(','):
            token = token.strip()
            try:

                if value_parser == TIME_PARSER:
                    (parsed_value, fractional_digit_count) = value_parser.parse(token)
                    self.max_fractional_time_digits = max(self.max_fractional_time_digits,
                                                          fractional_digit_count)
                else:
                    parsed_value = value_parser.parse(token)

                if protocol:
                    row_inputs.add_combinatorial_assay_metadata(protocol.pk, metadata_type_pk,
                                                                parsed_value)
                else:
                    row_inputs.add_combinatorial_line_metadata(metadata_type_pk,
                                                               parsed_value)
            except ValueError:
                cell_num = '%(row_num)d%(col_letter)s' % {
                    'row_num': row_num,
                    'col_letter': get_column_letter(col_index+1)}
                logger.warning(
                    'ValueError parsing token "%(token)s" from cell content "%(value)s" in '
                    '%(cell)s' % {
                        'token': token,
                        'value': cell_content,
                        'cell': cell_num,
                    }
                )
                bad_value = '%(token)s ((cell_num)s)' % {
                    'token': token,
                    'cell_num': cell_num,
                }
                self.importer.add_error(error_category, error_key, bad_value)
                break


class JsonInputParser(CombinatorialInputParser):
    """
    Parses / verifies JSON input for combinatorial line creation. Note that instances of this
    class maintain a cache of protocols and metadata types defined in the database, so the
    lifecycle of each instance should be short.
    """
    def __init__(self, protocols_by_pk, line_metadata_types_by_pk, assay_metadata_types_by_pk):
        super(JsonInputParser, self).__init__(protocols_by_pk, line_metadata_types_by_pk,
                                              assay_metadata_types_by_pk)
        self.protocols_by_pk = protocols_by_pk
        self.line_metadata_types_by_pk = line_metadata_types_by_pk
        self.assay_metadata_types_by_pk = assay_metadata_types_by_pk
        self.max_fractional_time_digits = 0
        self.parsed_json = None

    def parse(self, stream, importer):

        combinatorial_inputs = []
        self.importer = importer

        if importer.errors:
            return None

        ###########################################################################################
        # Once validated, parse the JSON string into a Python dict or list
        # if validation succeeded, extract the naming strategy from the JSON, then pass the rest
        # as parameters to CombinatorialDescriptionInput. Also cache for possible later use by
        # client code (e.g. in err emails)

        parsed_json = json.loads(stream)
        self.parsed_json = parsed_json

        if not parsed_json:
            return None

        max_decimal_digits = 0

        # tolerate either a sequence of CombinatorialDescriptionInput or a single one
        if not isinstance(parsed_json, Sequence):
            parsed_json = [parsed_json]

        for value in parsed_json:

            # work around reserved 'description' keyword in JSON schema
            description = value.pop('desc', None)

            # convert string-based keys required by JSON into their numeric equivalents
            # TODO: consider casting values too
            common_line_metadata = _copy_to_numeric_keys(value.pop('common_line_metadata', {}))
            combinatorial_line_metadata = _copy_to_numeric_keys(
                    value.pop('combinatorial_line_metadata', {}))
            protocol_to_assay_metadata = _copy_to_numeric_keys(
                    value.pop('protocol_to_assay_metadata', {}))
            protocol_to_combinatorial_metadata = _copy_to_numeric_keys(
                    value.pop('protocol_to_combinatorial_metadata', {}))

            naming_strategy = None
            naming_elements = value.pop('name_elements', None)
            if naming_elements:
                naming_strategy = AutomatedNamingStrategy(
                    self.line_metadata_types_by_pk,
                    self.assay_metadata_types_by_pk,
                    self.assay_time_meta_pk
                )
                elements = _copy_to_numeric_elts(naming_elements['elements'])
                abbreviations = _copy_to_numeric_keys(naming_elements['abbreviations'])

                naming_strategy.elements = elements
                naming_strategy.abbreviations = abbreviations
                naming_strategy.verify_naming_elts(importer)
            else:
                base_name = value.pop('base_name')
                naming_strategy = _ExperimentDescNamingStrategy(self.assay_time_meta_pk)
                naming_strategy.base_line_name = base_name

            try:
                # just pass the JSON as initializer arguments. Won't verify the internal
                # structure/expected data types, but for starters that's probably a safe bet
                combo_input = CombinatorialDescriptionInput(
                    naming_strategy, description=description,
                    common_line_metadata=common_line_metadata,
                    combinatorial_line_metadata=combinatorial_line_metadata,
                    protocol_to_assay_metadata=protocol_to_assay_metadata,
                    protocol_to_combinatorial_metadata=protocol_to_combinatorial_metadata,
                    **value
                )

                # inspect JSON input to find the maximum number of decimal digits in the user input
                if self.assay_time_meta_pk:
                    for protocol, assay_metadata in protocol_to_assay_metadata.items():
                        time_values = assay_metadata.get(self.assay_time_meta_pk, [])
                        for time_value in time_values:
                            str_value = str(time_value)
                            if str_value != str((int(float(time_value)))):
                                decimal_digits = len(str_value) - str_value.find('.') - 1
                                max_decimal_digits = max(max_decimal_digits, decimal_digits)

                naming_strategy.combinatorial_input = combo_input
                combinatorial_inputs.append(combo_input)
            except RuntimeError as rte:
                logger.exception('Unexpected parse error')
                self.importer.add_error(INTERNAL_EDD_ERROR_TITLE, PARSE_ERROR, str(rte))

        # verify primary key inputs from the JSON are for the expected MetaDataType context,
        # and that they exist, since there's no runtime checking for this at database item
        # creation time when they get (necessarily) shoved into the hstore field. Note that this is
        # an non-ideal comparison of database fields cached in memory, but should be an acceptable
        # risk of having a stale cache of these values, which should be recently gathered and very
        # unlikely to be stale.

        for combo_input in combinatorial_inputs:
            combo_input.verify_pks(
                self.line_metadata_types_by_pk,
                self.assay_metadata_types_by_pk,
                self.protocols_by_pk,
                self.importer,
            )

        # TODO: verify ICE strains are provided for every input if required
        # if self.require_strains:
        #     for combo_input in combinatorial_inputs:
        #         if not combo_input.combinatorial_strain_id_groups:
        #             add_parse_error(errors, 'strains required for all lines')
        #
        #         elif isinstance(combo_input.combinatorial_strain_id_groups, Sequence):
        #             for strain_id_group in combo

        # consistently use decimal or integer time in assay names based on whether any fractional
        # input was provided
        for combo_input in combinatorial_inputs:
            combo_input.fractional_time_digits = max_decimal_digits

        return combinatorial_inputs


def _copy_to_numeric_elts(input_list):
    converted_list = []
    for index, element in enumerate(input_list):
        try:
            int_value = int(element)
            converted_list.append(int_value)
        except ValueError:
            converted_list.append(element)

    return converted_list


def _copy_to_numeric_keys(input_dict):
    converted_dict = {}
    for key, value in input_dict.iteritems():

        # if value is a nested dict, do the same work on it
        if isinstance(value, dict):
            value = _copy_to_numeric_keys(value)
        try:
            int_value = int(key)
            converted_dict[int_value] = value
        except ValueError:
            converted_dict[key] = value
    return converted_dict
